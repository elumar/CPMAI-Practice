question_id,phase,tier,task_group,question_number,question_stem,option_a,option_b,option_c,option_d,correct_answer,answer_explanation,distractor_note,discrimination_point
CPMAI-P1-20250226-1842,Phase I - Business Understanding,1,Determine Business Objectives,1,"According to the CPMAI framework, which task is responsible for identifying the specific business metrics that will define project success?",Cost-Benefit Analysis,Determine Business Success Criteria,Business Feasibility,AI System Performance and Operation,B,Determine Business Success Criteria is the task specifically focused on defining the metrics and targets that indicate business objectives have been met.,"AI System Performance and Operation is tempting because it involves defining performance metrics, but those are technical and KPI thresholds - the task specifically responsible for identifying business success metrics is Determine Business Success Criteria.", - 
CPMAI-P1-20250226-2957,Phase I - Business Understanding,2,Cognitive Project Requirements,2,A project team is planning an AI initiative to automate customer service responses. They need to identify whether the solution requires natural language understanding or simple pattern matching. Which Phase I task addresses this specific need?,Assess Situation,Determine Business Objectives,AI Pattern Identification,Resource Requirements,C,AI Pattern Identification is the task within Cognitive Project Requirements that identifies the specific types of cognitive capabilities the AI solution needs.,"Assess Situation is tempting because it involves understanding context, but it focuses on resources and schedule, not cognitive requirements.", - 
CPMAI-P1-20250226-5123,Phase I - Business Understanding,2,Trustworthy AI Requirements,3,"A healthcare AI project team has documented Required Ethical AI Considerations, Required AI Transparency Considerations, Required AI Explainability Considerations, and Required Compliance With Regulations and Laws. The project sponsor asks whether they can move to Phase II. According to CPMAI, which additional Trustworthy AI task must be completed before the Go/No-Go decision?",Required AI Transparency Considerations,AI Failure Modes,Required AI Explainability Considerations,Required Compliance With Regulations and Laws,B,AI Failure Modes must be identified during Phase I to understand how the system could fail and what safeguards are needed before project approval.,"Required Compliance With Regulations and Laws is tempting because healthcare has strict regulations, but that task was explicitly completed in the scenario - AI Failure Modes is the only Trustworthy AI task not yet addressed.", - 
CPMAI-P1-20250226-6478,Phase I - Business Understanding,1,AI System Performance and Operation,4,Which two sets of performance values must be defined during Phase I according to the AI System Performance and Operation task group?,Training and Test Data Requirements,Acceptable Model Performance Values and Acceptable KPI Performance Values,Resource Requirements and Schedule Requirements,Cognitive Requirements and AI Pattern Identification,B,Acceptable Model Performance Values (technical) and Acceptable KPI Performance Values (business) are the two performance definition tasks in this task group.,"Training and Test Data Requirements is tempting but belongs to Phase II Data Understanding, not Phase I performance definition.", - 
CPMAI-P1-20250226-7231,Phase I - Business Understanding,3,AI Go/No-Go,5,"A project team has completed all Phase I tasks. Business feasibility is strong, and data appears available. However, the organization lacks data scientists with experience in the required technique. According to CPMAI, what is the correct outcome of the AI Go/No-Go task?",Proceed to Phase II - business and data feasibility are sufficient,Return to Determine Business Objectives to redefine the project scope,No-Go decision based on execution feasibility failure,Escalate to governance for a waiver to proceed without execution capability,C,Execution Feasibility is one of the three required feasibility assessments in the AI Go/No-Go task. Lack of skilled resources constitutes an execution feasibility failure.,"Proceed to Phase II is tempting because business and data feasibility are strong, but the Go/No-Go gate requires all three feasibility types (business, data, execution) to pass.","The critical distinction is that AI Go/No-Go requires three independent feasibility assessments - business, data, and execution - and all three must pass to proceed."
CPMAI-P1-20250226-3891,Phase I - Business Understanding,2,Trustworthy AI Requirements,6,"A financial services project team has documented the required model explainability approach and confirmed compliance with banking regulations. During review, a stakeholder asks how the team will handle situations where the model produces unexplainable results. Which Trustworthy AI requirement remains incomplete?",Required Ethical AI Considerations,Required AI Transparency Considerations,AI Failure Modes,Use of Trustworthy AI Framework,C,"AI Failure Modes requires identifying and planning for situations where the AI system fails or produces unexplainable results, which is precisely what the stakeholder is asking about.","Required AI Transparency Considerations is tempting because it relates to openness about how the model works, but failure modes specifically address what happens when things go wrong.", - 
CPMAI-P1-20250226-4562,Phase I - Business Understanding,1,Determine Business Objectives,7,"Within the Determine Business Objectives task group, which task directly precedes and informs the Cost-Benefit Analysis?",Assess Situation,Determine Business Objectives,Determine Business Success Criteria,Cognitive Project Requirements,B,"Determine Business Objectives establishes what the project aims to achieve, which is necessary input for calculating the costs and benefits of achieving those objectives.","Determine Business Success Criteria is tempting because it is sequentially adjacent to Cost-Benefit Analysis within the same task group, but it follows rather than precedes it - Determine Business Objectives establishes the goals that Cost-Benefit Analysis then evaluates.", - 
CPMAI-P1-20250226-5012,Phase I - Business Understanding,2,Assess Situation,8,"A project manager has identified that the AI initiative requires three data scientists and must be completed before the end of Q4. According to CPMAI, which Phase I task group is being executed?",Determine Business Objectives,Cognitive Project Requirements,Assess Situation,AI System Performance and Operation,C,"Assess Situation includes both Resource Requirements and Schedule Requirements tasks, which address staffing needs and timeline constraints.","AI System Performance and Operation is tempting because it involves defining performance values, but resource and schedule planning belong specifically to Assess Situation.", - 
CPMAI-P1-20250226-6345,Phase I - Business Understanding,1,Trustworthy AI Requirements,9,What is the priority level assigned to Trustworthy AI Requirements in the CPMAI framework?,Medium,High,Critical,Low,C,"Trustworthy AI Requirements is designated as Critical priority in the framework because it contains heavily tested CPMAI-specific concepts like ethics, explainability, and compliance.","High is tempting because many Phase I tasks are important, but the source material explicitly labels Trustworthy AI Requirements as Critical.", - 
CPMAI-P1-20250226-7128,Phase I - Business Understanding,3,Cognitive Project Requirements,10,A team has documented that their AI system needs to identify fraud patterns (pattern recognition) and recommend investigation priorities (decision support). The project manager believes this satisfies Cognitive Project Requirements. A reviewer disagrees. What is missing?,AI Pattern Identification - both requirements are patterns,The team has not identified whether the cognitive requirements are achievable with available data,The team should return to Phase II to confirm that available data supports both fraud pattern recognition and decision support before completing Cognitive Project Requirements,The team has not completed AI Pattern Identification and Cognitive Requirements as separate tasks - they have only done pattern identification,D,Cognitive Project Requirements is a task group containing two tasks: Cognitive Requirements (documenting all cognitive needs) and AI Pattern Identification (specifying pattern types). The team only performed pattern identification.,"Option B is tempting because data availability is always relevant, but the reviewer's concern is a Phase I task completion issue - Cognitive Project Requirements contains two separate tasks and the gap is incomplete Phase I documentation, not a data availability question requiring Phase II.",The distinction is that Cognitive Project Requirements is a task group containing two separate tasks; completing one does not complete the task group.
CPMAI-P1-20250226-8234,Phase I - Business Understanding,1,AI Go/No-Go,11,Which three feasibility assessments are required in the AI Go/No-Go task?,"Technical, Operational, and Financial","Business, Data, and Execution","Resource, Schedule, and Budget","Model, Data, and Infrastructure",B,"The AI Go/No-Go task explicitly requires Business Feasibility, Data Feasibility, and Execution Feasibility assessments.","Technical, Operational, and Financial is tempting because these are common project feasibility dimensions, but CPMAI specifies the three listed in the framework.", - 
CPMAI-P1-20250226-9456,Phase I - Business Understanding,2,Determine Business Objectives,12,"A retail company wants to use AI to reduce inventory waste. The project team defines success as ""fewer write-offs"" but cannot quantify the target. According to CPMAI, what is the primary issue with this approach?",The team skipped Cost-Benefit Analysis,The team has not defined measurable Business Success Criteria,The team should have started with Cognitive Project Requirements,The team needs to complete Assess Situation first,B,"Determine Business Success Criteria requires specific, measurable targets. ""Fewer write-offs"" is not measurable without a baseline and target percentage or value.","The team skipped Cost-Benefit Analysis is tempting because cost-benefit is important, but the immediate gap is the lack of measurable success criteria to inform that analysis.", - 
CPMAI-P1-20250226-2671,Phase I - Business Understanding,1,AI System Performance and Operation,13,"According to the framework, what is the purpose of defining Acceptable Model Performance Values during Phase I?",To set technical accuracy thresholds that the model must meet during evaluation,To determine which algorithm to use in Phase IV,To establish data quality requirements for Phase II,To calculate the ROI of the AI investment,A,"Acceptable Model Performance Values define the technical thresholds (accuracy, precision, recall, etc.) that the model must achieve, linking Phase I business goals to Phase V evaluation.","To determine which algorithm to use in Phase IV is tempting because performance relates to algorithm choice, but the purpose is setting evaluation thresholds, not selecting techniques.", - 
CPMAI-P1-20250226-1539,Phase I - Business Understanding,2,Trustworthy AI Requirements,14,A team building a hiring assistant AI has documented compliance with equal opportunity laws and defined how the model's decisions will be explained to candidates. A compliance officer asks how the team will ensure candidates know they are interacting with an AI. Which Trustworthy AI requirement addresses this?,Required AI Transparency Considerations,Required AI Explainability Considerations,Required Ethical AI Considerations,AI Failure Modes,A,Required AI Transparency Considerations addresses disclosure that an AI system is being used and openness about system capabilities and limitations.,"Required AI Explainability Considerations is tempting because it relates to explaining decisions, but transparency specifically covers disclosure of AI presence and system boundaries.", - 
CPMAI-P1-20250226-6782,Phase I - Business Understanding,3,AI Go/No-Go,15,"A project passes Business Feasibility and Execution Feasibility. Data Feasibility is marginal - the available data has significant gaps but could potentially be augmented. The sponsor wants to proceed to Phase II to ""explore data options."" What does CPMAI indicate?",Proceed - Phase II is exactly where data exploration occurs,No-Go - Data Feasibility must pass before leaving Phase I,Proceed with a conditional approval and data quality checkpoint,Return to Determine Business Objectives to scope down the project,B,The AI Go/No-Go gate requires all three feasibility assessments to pass before exiting Phase I. Marginal data feasibility constitutes a failure of Data Feasibility.,"Proceed - Phase II is exactly where data exploration occurs is the trap answer - while Phase II does explore data, the Go/No-Go gate must be passed to enter Phase II, and it requires feasibility confirmation, not exploration commitment.","The distinction is that Data Feasibility in Phase I asks ""can we get sufficient data?"" while Phase II Data Understanding asks ""what exactly is in the data we have?"" - feasibility must be confirmed before exploration begins."
CPMAI-P1-20250226-3348,Phase I - Business Understanding,1,Cognitive Project Requirements,16,"Which task group in Phase I is specifically responsible for identifying whether the AI solution requires perception, reasoning, or natural language capabilities?",Determine Business Objectives,Assess Situation,Cognitive Project Requirements,Trustworthy AI Requirements,C,"Cognitive Project Requirements contains AI Pattern Identification, which identifies the specific cognitive capabilities the AI solution requires.","Determine Business Objectives is tempting because objectives inform requirements, but the specific identification of cognitive capability types belongs to Cognitive Project Requirements.", - 
CPMAI-P1-20250226-9921,Phase I - Business Understanding,2,Assess Situation,17,A project team has identified that they need access to a specialized external dataset and that the project must deliver results before the annual shareholder meeting. Which two Assess Situation tasks are being performed?,Determine Business Objectives and Cost-Benefit Analysis,Resource Requirements and Schedule Requirements,Data Feasibility and Business Feasibility,AI Pattern Identification and Cognitive Requirements,B,"Resource Requirements covers needs like specialized data access, and Schedule Requirements covers timeline constraints like the shareholder meeting deadline.","Data Feasibility and Business Feasibility is tempting because these are feasibility types, but the scenario describes resource needs and timeline, which belong to Assess Situation tasks.", - 
CPMAI-P1-20250226-4087,Phase I - Business Understanding,1,Trustworthy AI Requirements,18,"According to the framework, which Trustworthy AI task ensures the project has identified relevant laws and regulations that apply to the AI system?",Required Ethical AI Considerations,Required AI Transparency Considerations,Required Compliance With Regulations and Laws,AI Failure Modes,C,Required Compliance With Regulations and Laws is the specific task addressing legal and regulatory requirements for the AI system.,"Required Ethical AI Considerations is tempting because ethics and compliance are related, but they are separate tasks addressing different concerns.", - 
CPMAI-P1-20250226-5753,Phase I - Business Understanding,2,Determine Business Objectives,19,"A manufacturing company wants to implement predictive maintenance. The team documents that the objective is to ""reduce unplanned downtime."" During review, a stakeholder asks how much reduction is enough and how it will be measured. Which Determine Business Objectives subtask is missing?",Determine Business Objectives,Cost-Benefit Analysis,Determine Business Success Criteria,Business Feasibility,C,Determine Business Success Criteria establishes the specific metrics and targets that define whether the business objective has been achieved.,"Determine Business Objectives is tempting because it sounds like the same thing, but the task group contains separate subtasks: setting the objective and defining how success is measured.", - 
CPMAI-P1-20250226-6681,Phase I - Business Understanding,3,AI System Performance and Operation,20,"A project team has defined Acceptable Model Performance Values at 95% accuracy. They have not defined Acceptable KPI Performance Values, arguing that KPIs will be addressed in Phase V during KPI Measurement. According to CPMAI, is this approach correct?","Yes - KPIs are measured in Phase V, so defining them in Phase I is premature",No - both model performance and KPI performance values must be defined in Phase I,Yes - model performance values are the only performance definition needed in Phase I,No - KPI values should be defined in Phase II during Data Understanding,B,AI System Performance and Operation requires defining both Acceptable Model Performance Values and Acceptable KPI Performance Values during Phase I to link technical and business success criteria from the start.,"Yes - KPIs are measured in Phase V, so defining them in Phase I is premature is the trap - while KPI Measurement occurs in Phase V, the target values for those KPIs must be established in Phase I to guide the entire project.","The distinction is between setting performance targets (Phase I) and measuring actual performance against those targets (Phase V) - both are required, and they occur in different phases."
CPMAI-P1-20260226-2824,Phase I - Business Understanding,2,AI System Performance and Operation,1,"A team wants to add a generative AI feature to a customer support app. Engineers report that a prototype model performs well in lab testing, but the product owner realizes no one has defined what level of KPI performance is acceptable for the live customer experience. Stakeholders disagree on whether current accuracy is good enough. In Phase I, which task sets the acceptable KPI threshold?",Verify Data Quality,Determine Business Success Criteria,KPI Measurement,Acceptable KPI Performance Values,D,Acceptable KPI Performance Values in AI System Performance and Operation defines the acceptable KPI threshold before execution begins.,"Option B is tempting because it sounds similar, but success criteria are broader business outcomes while acceptable KPI values are the numeric thresholds used to judge KPI performance.", - 
CPMAI-P1-20260226-1409,Phase I - Business Understanding,1,Determine Business Objectives,2,"Within Phase I, which subtask in Determine Business Objectives evaluates whether expected benefits justify the costs?",Determine Business Success Criteria,Cost-Benefit Analysis,Business Feasibility,Acceptable KPI Performance Values,B,Cost-Benefit Analysis is the Determine Business Objectives subtask used to weigh benefits against costs in Phase I.,"Option A is tempting, but success criteria define what success looks like, not whether the investment is worth it.", - 
CPMAI-P1-20260226-5506,Phase I - Business Understanding,2,AI Go/No-Go,3,"A team can build an ML feature, but running it will require new on-call coverage and ongoing operational support that is not currently staffed. In Phase I, which AI Go/No-Go feasibility area is most directly at risk?",Resource Requirements,Data Feasibility,Execution Feasibility,AI Failure Modes,C,Execution Feasibility in AI Go/No-Go tests whether the organization can realistically execute and sustain the solution.,"Option A is tempting, but Resource Requirements is part of Assess Situation and does not replace the AI Go/No-Go feasibility gate. Option D is tempting because failure modes relate to operational risk, but AI Failure Modes captures how the system can fail, not whether the organization can sustain operations.", - 
CPMAI-P1-20260226-5012,Phase I - Business Understanding,1,Trustworthy AI Requirements,4,"In Phase I, which task in Trustworthy AI Requirements addresses meeting legal and regulatory obligations for the AI solution?",Required Compliance With Regulations and Laws,Required Ethical AI Considerations,Required AI Transparency Considerations,Required AI Security Considerations,A,Required Compliance With Regulations and Laws is the Trustworthy AI Requirements task that captures legal and regulatory obligations in Phase I.,"Option B is tempting, but ethical considerations address value-based concerns rather than legal compliance obligations. Option D is tempting because security is regulatory-adjacent, but it focuses on protecting the system, not meeting legal and regulatory requirements.", - 
CPMAI-P1-20260226-4657,Phase I - Business Understanding,3,Cognitive Project Requirements,5,"A support team says, ""We think AI could help somewhere in this workflow, but we are not sure what to automate or what type of AI fits."" In Phase I, what is the best next step?",Cognitive Requirements to define the needed cognitive capability in detail,AI Pattern Identification to match the problem to an AI pattern,Use of Trustworthy AI Framework to evaluate readiness,Business Feasibility to confirm the initiative is viable,B,AI Pattern Identification in Cognitive Project Requirements is used to recognize where AI fits and what pattern applies before detailing the cognitive capability.,"Option A is tempting because it is also in Cognitive Project Requirements, but defining cognitive requirements comes after you identify the AI pattern that fits the problem. Option D is tempting because feasibility is a valid Phase I concern, but the team first needs to identify the AI pattern before gating the initiative.",The trap is confusing pattern selection (AI Pattern Identification) with specifying capability details (Cognitive Requirements).
CPMAI-P1-20260226-3286,Phase I - Business Understanding,2,Assess Situation,6,"A program manager is asked to deliver a Phase I plan for a new AI initiative. Leadership needs a clear view of the people required, their availability, and the timeline before approving work. The PM is unsure whether this falls under business objectives or project planning. In Phase I, which task group addresses staffing and schedule planning?",Determine Business Objectives,Assess Situation,AI Go/No-Go,Cognitive Project Requirements,B,Assess Situation covers Resource Requirements and Schedule Requirements needed to plan the work in Phase I.,"Option C is tempting, but AI Go/No-Go is the feasibility gate and does not replace planning the resource and schedule needs. Option A is tempting because objectives and planning feel related, but Determine Business Objectives captures what the business wants to achieve, not what resources are needed.", - 
CPMAI-P1-20260226-2679,Phase I - Business Understanding,1,AI System Performance and Operation,7,"In Phase I, which task defines acceptable model performance targets for the AI system?",KPI Measurement,Determine Business Success Criteria,AI Failure Modes,Acceptable Model Performance Values,D,Acceptable Model Performance Values is the Phase I task that sets acceptable model performance targets before development.,"Option B is tempting because success criteria sound like performance targets, but they define desired business outcomes, not model performance thresholds. Option A is tempting because KPI Measurement is in the same task group, but it addresses measuring KPIs, not setting acceptable model performance values.", - 
CPMAI-P1-20260226-9935,Phase I - Business Understanding,2,AI Go/No-Go,8,"A team wants to launch an ML feature, but the needed data is not available and would require new collection agreements. In Phase I, which AI Go/No-Go check should drive the decision?",Verify Data Quality,Collect Initial Data,Data Feasibility,Determine Business Objectives,C,Data Feasibility in AI Go/No-Go determines whether the needed data can realistically be obtained for the solution.,"Option B is tempting because it is real work, but Collect Initial Data is a Phase II activity and comes after the Phase I feasibility decision.", - 
CPMAI-P1-20260226-2424,Phase I - Business Understanding,1,Assess Situation,9,Which pair of tasks belongs to the Phase I task group Assess Situation?,AI Failure Modes and Required Ethical AI Considerations,Cost-Benefit Analysis and Business Feasibility,Resource Requirements and Schedule Requirements,Acceptable Model Performance Values and KPI Measurement,C,Assess Situation is composed of Resource Requirements and Schedule Requirements in Phase I.,"Option B is tempting because it sounds like planning, but it mixes tasks from Determine Business Objectives and AI Go/No-Go task groups.", - 
CPMAI-P1-20260226-7912,Phase I - Business Understanding,2,Trustworthy AI Requirements,10,"A regulator may ask the team to explain why the AI system made a specific decision for a customer. In Phase I, which task directly captures this requirement?",Required AI Explainability Considerations,Required AI Transparency Considerations,AI Failure Modes,Business Feasibility,A,Required AI Explainability Considerations defines the need to explain AI decisions in Phase I.,"Option B is tempting because transparency and explainability are closely related. However, transparency is about disclosing that AI is being used and how the system works in general, while explainability is about explaining why the AI made a specific decision for a specific case - which is what the regulator is asking for.", - 
CPMAI-P1-20260226-1520,Phase I - Business Understanding,3,Determine Business Objectives,11,"A team has documented that the AI initiative should reduce average handle time by 15% and improve first-contact resolution. Separately, they recorded that the model's accuracy must stay above 92% and latency must remain under 200ms. A stakeholder asks whether both statements belong to the same Phase I task. What is the correct distinction?",The first statement defines acceptable KPI performance values; the second defines acceptable model performance values.,The first statement defines business success criteria; the second defines acceptable KPI performance values.,Both statements are business success criteria because they all measure project outcomes.,The first statement belongs to Assess Situation because it sets resource expectations for the team.,B,"Determine Business Success Criteria captures the desired business outcomes (handle time, first-contact resolution), while Acceptable KPI Performance Values sets the numeric thresholds for KPI performance. They are distinct Phase I tasks in different task groups.","Option A is the strongest trap. It correctly identifies the second statement as model performance values (accuracy, latency) but misclassifies the first statement. Handle time and first-contact resolution are business outcomes, not KPI thresholds - the KPI threshold would be the acceptable numeric value for measuring those outcomes.",Business success criteria define desired outcomes (what we want to achieve); acceptable KPI values set numeric thresholds (how we measure achievement); acceptable model performance values set technical targets (how the model must behave).
CPMAI-P1-20260226-1488,Phase I - Business Understanding,1,Trustworthy AI Requirements,12,"In Phase I, which Trustworthy AI Requirements task ensures the team adopts and applies a structured, organization-wide approach to responsible AI throughout the project lifecycle?",Data Feasibility,AI Failure Modes,Use of Trustworthy AI Framework,Cost-Benefit Analysis,C,Use of Trustworthy AI Framework is the Trustworthy AI Requirements task that defines applying a trustworthy AI framework in Phase I.,"Option B is tempting, but failure modes is a separate task focused on how the system can fail, not on applying a responsible AI framework.", - 
CPMAI-P1-20260226-2535,Phase I - Business Understanding,2,Cognitive Project Requirements,13,"A sponsor says, ""We need the system to read customer messages and route them to the right support path."" The team is debating what cognitive capability is required before choosing a solution approach. In Phase I, which task applies?",AI Pattern Identification,Cognitive Requirements,Collect Initial Data,Resource Requirements,B,Cognitive Requirements defines the needed cognitive capability in Phase I before selecting or designing the solution.,"Option A is tempting because it is nearby, but pattern identification is about matching a problem to an AI pattern, not defining the cognitive capability itself.", - 
CPMAI-P1-20260226-4582,Phase I - Business Understanding,1,Trustworthy AI Requirements,14,"In Phase I, which task is responsible for identifying how an AI system could fail?",Schedule Requirements,Required Ethical AI Considerations,Acceptable Model Performance Values,AI Failure Modes,D,AI Failure Modes is the Trustworthy AI Requirements task that captures how the system could fail in Phase I.,"Option B is tempting, but ethics focuses on value-based considerations rather than failure scenarios.", - 
CPMAI-P1-20260226-4811,Phase I - Business Understanding,3,AI Go/No-Go,15,"A product owner wants the team to start collecting sample data immediately to ""see what we can do."" You have not yet confirmed whether the data can realistically be obtained at the needed scope. In Phase I, what is the correct next step?",Collect Initial Data,Verify Data Quality,Data Feasibility,Enhance and Augment Data,C,Data Feasibility in AI Go/No-Go is the Phase I check that determines whether required data can realistically be obtained before moving into Phase II work.,"Option A is tempting, but Collect Initial Data is a Phase II activity and should follow the Phase I data feasibility decision.",The discrimination is between Phase I feasibility gating (Data Feasibility) and Phase II execution work (Collect Initial Data and Verify Data Quality).
CPMAI-P1-20260226-9279,Phase I - Business Understanding,2,Determine Business Objectives,16,"A sponsor says, ""Let’s build the model first and figure out success later."" In Phase I, what is the best response aligned to the framework?",Define Business Success Criteria before proceeding,Set Acceptable Model Performance Values and start training,Lock Resource Requirements and begin execution,Complete AI Go/No-Go and proceed to Phase II,A,Determine Business Success Criteria in Determine Business Objectives establishes what success means before later work proceeds.,"Option B is tempting, but model performance values are not a substitute for defining business success criteria. Option D is tempting because it suggests forward progress, but the Go/No-Go gate cannot be completed without first defining what success means.", - 
CPMAI-P1-20260226-1434,Phase I - Business Understanding,1,Cognitive Project Requirements,17,"In Phase I, which task focuses on recognizing where AI can be applied by matching the problem to an AI pattern?",Acceptable KPI Performance Values,Cognitive Requirements,Business Feasibility,AI Pattern Identification,D,AI Pattern Identification is the Cognitive Project Requirements task used to match a problem to a suitable AI pattern in Phase I.,"Option B is tempting, but cognitive requirements specify the needed cognitive capability after identifying the applicable AI pattern.", - 
CPMAI-P1-20260226-4257,Phase I - Business Understanding,2,Assess Situation,18,"A project needs two subject matter experts for reviews and one analyst for weekly reporting, but the team is not sure if those people are available. In Phase I, which task addresses this question?",Resource Requirements,Schedule Requirements,Business Feasibility,Use of Trustworthy AI Framework,A,Resource Requirements in Assess Situation defines the resource needs and availability considerations in Phase I.,"Option B is tempting, but schedule requirements address timing, not staffing availability.", - 
CPMAI-P1-20260226-9928,Phase I - Business Understanding,1,AI Go/No-Go,19,Which list includes all feasibility checks in the Phase I task group AI Go/No-Go?,"Business Feasibility, Data Feasibility, Execution Feasibility","Resource Requirements, Schedule Requirements, Cost-Benefit Analysis","Collect Initial Data, Verify Data Quality, Data Selection","Acceptable Model Performance Values, KPI Measurement, Model Monitoring and Maintenance",A,"AI Go/No-Go in Phase I evaluates Business Feasibility, Data Feasibility, and Execution Feasibility.","Option B is tempting because it sounds like feasibility, but it mixes tasks from other Phase I task groups.", - 
CPMAI-P1-20260226-7873,Phase I - Business Understanding,3,AI System Performance and Operation,20,"A pilot team sets a target for self-service adoption improvement, but they have not agreed on acceptable model behavior like latency and error rates. In Phase I, which task should be completed next?",Model Test and Validation Design,Acceptable KPI Performance Values,Determine Business Success Criteria,Acceptable Model Performance Values,D,Acceptable Model Performance Values in AI System Performance and Operation defines acceptable model behavior such as performance targets before development proceeds.,"Option B is tempting because it is also in the same task group, but KPI values reflect business KPI thresholds, not model behavior like latency and error rates.",The discrimination is between model behavior targets (Acceptable Model Performance Values) and business KPI thresholds (Acceptable KPI Performance Values).
CPMAI-P2-20250226-4721,Phase II - Data Understanding,1,Inventory Data Sources and Acquire Data,1,"Which task in Phase II involves creating a catalog of all data locations, formats, and access methods?",Perform Data Exploration,Inventory Data Sources and Acquire Data,Document Data Understanding Findings,Perform Initial Data Quality Assessment,B,Inventory Data Sources and Acquire Data is the Phase II task focused on systematically identifying and cataloging all available data sources before acquisition.,"Option D (Data Quality Assessment) is tempting because quality assessment follows inventory, but inventory itself is the cataloging task.", - 
CPMAI-P2-20250226-8392,Phase II - Data Understanding,2,Perform Data Exploration,2,"A project manager receives three new datasets for a customer churn prediction project at a mid-size telecom company. The marketing VP wants to understand which customer segments are most at risk. Before assessing data quality, the PM runs basic descriptive statistics, visualizes distributions, and examines relationships between variables like tenure, monthly charges, and contract type. Which Phase II task is being performed?",Perform Data Exploration,Inventory Data Sources and Acquire Data,Perform Initial Data Quality Assessment,Document Data Understanding Findings,A,"The described activities - descriptive statistics, visualizations, examining relationships - are the core activities of the Perform Data Exploration task in Phase II.","Option C (Data Quality Assessment) is tempting because exploration often reveals quality issues, but quality assessment is a separate task with specific dimensions and metrics.", - 
CPMAI-P2-20250226-1537,Phase II - Data Understanding,3,Perform Initial Data Quality Assessment,3,"A data scientist reviewing a customer demographics dataset notices that 15% of the age field contains the value '0'. The business team confirms that age is a required field and the intake form does not allow blank submissions — agents were instructed to enter '0' when the customer declined to share their age. According to CPMAI, which data quality dimension is most directly at issue?",Accuracy — the recorded values do not reflect the customers' true ages,Completeness — the '0' values represent absent real data,Validity — the values violate the expected range for age,Consistency — the encoding convention differs from other fields,B,"Completeness measures whether meaningful data values are present. Although '0' is technically populated, it is a surrogate for missing data — the real age was never captured. The values are placeholders, not measurements, making this a completeness issue.","Option A (Accuracy) is the strongest trap — '0' is clearly not the true age, which looks like an accuracy problem. However, the root cause is that no real value was collected, which is a completeness failure. Option C (Validity) is also tempting because '0' may violate range rules, but the underlying issue is absence of data, not a value that was entered incorrectly.","The distinction between completeness (data was never captured, even if a placeholder exists) and accuracy (data was captured but the recorded value is wrong) is the key — placeholder values that mask missing data are completeness issues, not accuracy issues."
CPMAI-P2-20250226-6294,Phase II - Data Understanding,2,Document Data Understanding Findings,4,"A project team completes data exploration and initial quality assessment. Before moving to Phase III, they create a report detailing data sources, field descriptions, quality issues identified, and preliminary insights. What is the primary purpose of this documentation?",To provide input to Phase III Data Preparation activities,To serve as the final project archive,To replace the need for a data dictionary,To fulfill regulatory compliance requirements,A,Document Data Understanding Findings captures all Phase II outputs specifically to inform and guide the Data Preparation activities in Phase III.,"Option B is tempting because documentation creates records, but Phase II findings are inputs to the next phase, not final archives.", - 
CPMAI-P2-20250226-7483,Phase II - Data Understanding,1,Inventory Data Sources and Acquire Data,5,What is the priority level of the Inventory Data Sources and Acquire Data task within Phase II?,Critical,High,Low,Medium,D,"According to CPMAI priority weighting, Inventory Data Sources and Acquire Data is a Medium-priority task, while Data Exploration and Quality Assessment are Critical.","Option A (Critical) is tempting because data acquisition seems fundamental, but the framework prioritizes exploration and quality assessment activities higher.", - 
CPMAI-P2-20250226-3916,Phase II - Data Understanding,3,Perform Data Exploration,6,"A project manager finds that customer transaction data shows a highly skewed distribution with 90% of transactions under $100 and 10% ranging from $1,000 to $50,000. Which statement about handling this finding in Phase II is most accurate?",The data should be normalized immediately to address the skew,The dataset should be rejected because skewed data invalidates modeling,This finding should be documented as an exploration insight for Phase III preparation decisions,"Outliers above $1,000 should be removed from the dataset",C,Documenting distribution characteristics (including skew) as exploration findings is the correct Phase II action - transformation decisions like normalization occur in Phase III.,"Option A is tempting because skew often requires transformation, but normalization is a Phase III preparation activity, not a Phase II action.",The boundary between Phase II (exploration/documentation) and Phase III (transformation/preparation) is the critical distinction - candidates often implement solutions in the wrong phase.
CPMAI-P2-20250226-5270,Phase II - Data Understanding,2,Perform Initial Data Quality Assessment,7,A healthcare analytics project discovers that patient blood pressure readings in the electronic medical records system are consistently 5-10 mmHg higher than readings taken manually during the same visits. Which data quality dimension is most relevant to this finding?,Completeness,Timeliness,Accuracy,Consistency,C,Accuracy measures whether data values correctly represent real-world values - the systematic deviation between recorded and manual readings indicates an accuracy issue.,"Option D (Consistency) is tempting because two sources disagree, but consistency typically refers to format/representation agreement, not correctness against ground truth.", - 
CPMAI-P2-20250226-2643,Phase II - Data Understanding,1,Document Data Understanding Findings,8,Which Phase II task group includes creating the Data Quality Report that catalogs issues found during initial assessment?,Inventory Data Sources and Acquire Data,Perform Data Exploration,Perform Initial Data Quality Assessment,Document Data Understanding Findings,D,"Document Data Understanding Findings is the Phase II task group responsible for formalizing all Phase II outputs into deliverables, including the Data Quality Report. While Perform Initial Data Quality Assessment identifies the issues, the Documentation task group owns the creation of the formal report artifact.","Option C is tempting because quality assessment identifies issues, but documenting them in a formal report is part of the Documentation task group.", - 
CPMAI-P2-20250226-9851,Phase II - Data Understanding,2,Inventory Data Sources and Acquire Data,9,"A retail analytics project requires customer transaction data from three different systems: point-of-sale, e-commerce platform, and loyalty program. The project manager discovers that the e-commerce platform data cannot be extracted in the required format. According to CPMAI, what should happen next?",Document the data access limitation as a finding,Proceed with only POS and loyalty data,Abandon the project due to incomplete data,Request IT build a custom extraction tool,A,Documenting data access limitations as findings is the correct Phase II action - this information informs Phase III planning and may trigger scope adjustments.,"Option B is tempting because projects often proceed with available data, but CPMAI requires documenting limitations before proceeding, not silently dropping sources.", - 
CPMAI-P2-20250226-4372,Phase II - Data Understanding,3,Perform Initial Data Quality Assessment,10,"A project team identifies that customer addresses in the database use inconsistent formats: some have ZIP+4, some have 5-digit ZIP, some include apartment numbers in different positions. According to CPMAI's data quality dimensions, this is primarily an issue of:",Accuracy,Consistency,Completeness,Timeliness,B,Consistency refers to whether data is represented in a uniform format and follows the same rules across records - inconsistent address formats are a classic consistency issue.,"Option A (Accuracy) is tempting because formatting affects data usability, but accuracy refers to correctness of content, not uniformity of representation.","The distinction between consistency (format uniformity) and accuracy (content correctness) is frequently tested - inconsistent formats can still be accurate, and consistent formats can still be wrong."
CPMAI-P2-20250226-7401,Phase II - Data Understanding,1,Perform Data Exploration,11,"In Phase II, what is the primary purpose of the Perform Data Exploration task before proceeding to Perform Initial Data Quality Assessment?","To understand data characteristics, distributions, and relationships that inform quality assessment",To transform raw data into a format suitable for modeling,To select the final set of features for the predictive model,To validate that the data meets the business success criteria defined in Phase I,A,"Perform Data Exploration in Phase II focuses on understanding the data through descriptive statistics, distributions, and relationship analysis. These exploration insights directly inform what to look for during the subsequent quality assessment.","Option B is tempting because exploration can reveal transformation needs, but data transformation is a Phase III Data Preparation activity. Option C is tempting because exploration reveals variable importance, but feature selection also occurs in Phase III.", - 
CPMAI-P2-20250226-6138,Phase II - Data Understanding,2,Document Data Understanding Findings,12,"A project team completes Phase II and prepares to present findings at the gate review. The sponsor asks why the documentation includes a section on ""Data Sources Not Acquired"" when those sources will not be used. What should the project manager explain?",The section was included by mistake and can be removed,Documenting inaccessible sources prevents future teams from wasting time pursuing the same sources,All potential data sources must be documented regardless of access,The section fulfills a regulatory requirement,B,Documenting data sources that could not be acquired is a valuable finding that prevents redundant efforts in future iterations or related projects.,"Option C is tempting because documentation is important, but CPMAI focuses on documenting findings that add value, not mandatory documentation of every possible source.", - 
CPMAI-P2-20250226-2947,Phase II - Data Understanding,2,Inventory Data Sources and Acquire Data,13,"A manufacturing analytics project needs sensor data from 200 machines. The project manager creates a spreadsheet listing each machine's data location, format, update frequency, and access contact. Which Phase II activity is being performed?",Data Quality Assessment,Data Exploration,Inventory Data Sources and Acquire Data,Document Data Understanding Findings,C,"Creating a systematic catalog of data sources with their attributes (location, format, frequency, contacts) is the Inventory Data Sources activity.","Option B (Data Exploration) is tempting because inventory precedes exploration, but exploration involves analyzing data content, not cataloging sources. Option D is tempting because the spreadsheet looks like documentation, but it is the inventory activity itself, not the formal documentation of findings.", - 
CPMAI-P2-20250226-8560,Phase II - Data Understanding,1,Perform Initial Data Quality Assessment,14,Which data quality dimension measures whether data values fall within acceptable predefined ranges?,Completeness,Consistency,Validity,Accuracy,C,"Validity assesses whether data values conform to defined rules, formats, or ranges - the scenario describes checking against acceptable ranges.","Option D (Accuracy) is tempting because range violations often indicate inaccuracy, but validity specifically tests conformance to rules, not correctness per se.", - 
CPMAI-P2-20250226-1024,Phase II - Data Understanding,3,Perform Data Exploration,15,A project manager runs correlation analysis on 50 variables in a customer behavior dataset and finds 15 statistically significant correlations at p < 0.05. The business sponsor wants to immediately build a model using these 15 variables. What does CPMAI indicate?,Proceed to Phase III with these 15 variables - statistical significance validates their importance,Move directly to Phase IV modeling with the correlated variables,Remove all non-correlated variables to streamline the dataset,Document the correlations but continue systematic exploration before Phase III,D,"Phase II requires complete exploration before transitioning to Phase III - documented correlations are exploration findings, not triggers to skip remaining exploration or move directly to modeling.","Option A is tempting because significant correlations seem valuable, but CPMAI requires completing all exploration activities before exiting Phase II, regardless of early findings.",The distinction between exploration findings and preparation decisions is critical - correlations are Phase II discoveries; variable selection happens in Phase III after all exploration is complete.
CPMAI-P2-20250226-5739,Phase II - Data Understanding,2,Perform Initial Data Quality Assessment,16,"A financial services project discovers that 5% of transaction records have negative dollar amounts where only positive amounts should exist. According to CPMAI, how should this be documented in Phase II?",As an accuracy issue - the values are incorrect for the business context,As a completeness issue - negative values indicate missing positive values,As a consistency issue - negative values conflict with expected format,As a validity issue - values violate the business rule requiring positive amounts,D,"Validity measures conformance to business rules — negative amounts violating the ""positive only"" rule is a validity issue, not an accuracy or consistency issue.","Option A (Accuracy) is tempting because negative amounts are incorrect, but validity specifically addresses rule conformance while accuracy addresses correctness against true values.", - 
CPMAI-P2-20250226-3907,Phase II - Data Understanding,1,Document Data Understanding Findings,17,"Which Phase II document typically contains field-by-field descriptions including data type, meaning, and source system?",Data Quality Report,Data Dictionary,Project Charter,Model Validation Report,B,The Data Dictionary is created during Phase II Documentation and contains detailed field-level metadata about each data element.,"Option A (Data Quality Report) is tempting because it is also a Phase II document, but it focuses on quality issues, not field definitions.", - 
CPMAI-P2-20250226-6452,Phase II - Data Understanding,2,Inventory Data Sources and Acquire Data,18,"A telecommunications company is acquiring customer call detail records for a network optimization project. The data source has 18 months of historical data, but the project only requires 12 months. According to CPMAI, what should the project manager document?",Only that the source was acquired successfully,That 6 months of data will be ignored,The full available history as a finding for potential future use,That the source should be rejected for providing too much data,C,Documenting that additional historical data is available (even if not immediately needed) is a valuable finding that may inform future project iterations or scope expansions.,"Option A is tempting because the acquisition succeeded, but CPMAI emphasizes documenting all relevant characteristics, not just acquisition status.", - 
CPMAI-P2-20250226-8135,Phase II - Data Understanding,3,Perform Initial Data Quality Assessment,19,"A project team finds that customer ages in a dataset range from 0 to 150, with 8% of records showing ages over 120. The business rule states valid ages must be between 0 and 120. However, further investigation reveals the over-120 records correspond to legitimate centenarians whose ages were entered correctly. Which quality dimension is most accurately represented by this situation?",The data has a validity issue because ages violate the business rule,The data has an accuracy issue because ages are correct despite rule violation,The data has no quality issues because the ages are correct,The data has both validity and accuracy considerations that should be documented separately,D,"This scenario illustrates why multiple dimensions must be assessed independently - the data is accurate (correct values) but invalid (violates business rules), requiring documentation of both findings.","Option A is tempting because rule violation is clear, but ignoring the accuracy finding would be incomplete documentation. Option B is tempting because the values are correct, but ignoring the rule violation misses a validity issue.","The independence of quality dimensions is a critical Phase II concept - a single value can be accurate but invalid, or valid but inaccurate, requiring separate assessment of each dimension."
CPMAI-P2-20250226-2971,Phase II - Data Understanding,1,Perform Data Exploration,20,Which Phase II output from Perform Data Exploration is used as direct input to the Perform Initial Data Quality Assessment task?,"Documented data characteristics including distributions, patterns, and relationships",The project charter approved by the sponsor,The final cleaned dataset ready for modeling,The feature selection report identifying variables for the model,A,"Perform Data Exploration produces documented data characteristics — distributions, patterns, and relationships — that directly inform what the quality assessment should examine. This handoff from exploration to quality assessment is a core Phase II workflow.","Option C is tempting because cleaned data sounds like an output, but cleaning occurs in Phase III, not Phase II. Option D is tempting because exploration reveals variable importance, but feature selection is a Phase III activity.", - 
CPMAI-P2-20260226-2679,Phase II — Data Understanding,2,Data quality,1,"At a fintech startup, the team finally gets access to two years of historical transaction logs from the payment processing vendor. A data engineer, under pressure from the product lead to hit a tight sprint deadline, starts deduplicating records and filling in missing values to ""save time"" before anyone has formally assessed the dataset's quality. The PM intervenes. In CPMAI Phase II, what should the team do first?",Select cognitive-relevant algorithm / modeling technique,Clean data,Collect Initial Data,Verify Data Quality,D,"Phase II requires Verify Data Quality to assess completeness, errors, and missing values before any cleansing work begins.","Option B is tempting, but Clean data is a Phase III Data Preparation task, while Phase II focuses on assessing and documenting quality issues first.", - 
CPMAI-P2-20260226-1520,Phase II — Data Understanding,1,Collect initial data,2,"In CPMAI Phase II, under which task group are the following tasks organized: Collect Initial Data, Describe Data, and Explore Data & Cognitive Data Requirements?",Machine learning model data requirements,Data quality,Collect initial data,Data Selection,C,"Collect Initial Data, Describe Data, and Explore Data & Cognitive Data Requirements sit under the Phase II task group Collect initial data.","Option D is tempting because it sounds similar, but Data Selection is listed under Phase III, not Phase II.", - 
CPMAI-P2-20260226-2424,Phase II — Data Understanding,3,Machine learning model data requirements,3,"A team is building a supervised learning model and is debating how to split data into training, validation, and test sets and whether to use k-fold cross-validation. Two options seem plausible: defining the split plan now or starting labeling work immediately. In CPMAI Phase II, which task is the best match for this decision?",Verify Data Quality,Label data,Training & Test Data Requirements,Select data,C,"Training & Test Data Requirements is the Phase II task that defines training, validation, and test dataset details, including split and cross-validation decisions.","TRAP NOTE: Option B is tempting, but Label data is a Phase III Data Preparation task; Phase II here is about specifying dataset requirements and evaluation splits.","The boundary is Phase II dataset planning (splits, validation approach) versus Phase III execution work like labeling and preparation."
CPMAI-P2-20260226-4811,Phase II — Data Understanding,2,Collect initial data,4,"A manufacturer wants to predict equipment failures. The team has sensor feeds, maintenance tickets, and vendor CSV exports, but nobody has documented where each dataset lives or how it will be pulled. Which Phase II task should the PM drive to create that inventory and capture access issues?",Collect Initial Data,Describe Data,Verify Data Quality,Select data,A,"Collect Initial Data is used in Phase II to acquire the identified datasets and document locations, acquisition methods, and problems encountered.","Option B is tempting, but Describe Data focuses on summarizing the properties of data already acquired, not the initial acquisition and access log.", - 
CPMAI-P2-20260226-3286,Phase II — Data Understanding,1,Data quality,5,"Which Phase II task produces the Data quality report as its primary output, documenting findings about the dataset's fitness for the intended use?",Training & Test Data Requirements,Explore Data & Cognitive Data Requirements,Enhance & Augment data,Verify Data Quality,D,"Verify Data Quality is the Phase II task dedicated to assessing completeness, errors, and missing values and documenting them in a Data quality report.","Option B is tempting, but Explore Data focuses on distributions and relationships; quality verification is a separate task producing the Data quality report.", - 
CPMAI-P2-20260226-9928,Phase II — Data Understanding,2,Machine learning model data requirements,6,"A telecom team is planning an on-device model for field technicians with limited connectivity. They need to document constraints like response time, limits on external data access, and what data can be captured on the device. Which Phase II task should they use?",Deploying Models for Production Environments,Edge Model Data Needs,Describe Data,Training & Test Data Requirements,B,"Edge Model Data Needs is the Phase II task that defines edge constraints, data capture requirements, and related operational constraints for models used at the edge.","Option A is tempting, but production deployment planning belongs to later operationalization work. Option D is tempting because it also addresses model data planning, but it covers splits and validation strategy, not edge-specific constraints.", - 
CPMAI-P2-20260226-7912,Phase II — Data Understanding,1,Collect initial data,7,Which output best matches what CPMAI expects from the Phase II task Describe Data?,"A report summarizing data format, quantity, field identities, and other surface features",A cleaned and deduped dataset ready for model training,A labeling guideline and annotation workflow,A trained baseline model with initial hyperparameters,A,"Describe Data produces a data description report that captures format, quantity, fields, and surface characteristics of the acquired data.","Option B is tempting, but cleaning and deduping are Phase III activities, not the Describe Data task in Phase II.", - 
CPMAI-P2-20260226-5506,Phase II — Data Understanding,3,Data quality,8,"A healthcare analytics team finds many missing values and duplicate records in patient encounter data. The sponsor wants the team to immediately impute missing values and remove duplicates so modeling can start. Under CPMAI, what is the most correct Phase II action?",Document the issues in the Data quality report and proceed to Phase III for remediation,Clean data and de-duplicate immediately to keep the timeline,Verify Data Quality and document the issues before deciding Phase III fixes,Escalate to the Phase II Go/No-Go gate to decide whether the data is viable,C,"Phase II centers on Verify Data Quality, which documents missingness, errors, and completeness before Phase III cleansing actions are chosen.","TRAP NOTE: Option A is the strongest trap — it correctly identifies documentation as a Phase II action and remediation as Phase III work, but it skips the critical step of verification and assessment. The correct answer emphasizes verifying and documenting quality issues before deciding what Phase III fixes to pursue. Option B is also tempting but Clean data is explicitly a Phase III task.","The distinction is Phase II verification and documentation of data issues versus Phase III remediation (cleansing, de-duplication, transformations)."
CPMAI-P2-20260226-1488,Phase II — Data Understanding,1,Collect initial data,9,"In Phase II, which task is primarily responsible for surfacing early findings, hypotheses, and patterns by examining the data's statistical properties and variable relationships?",Select data,Describe Data,Verify Data Quality,Explore Data & Cognitive Data Requirements,D,"Explore Data & Cognitive Data Requirements is the Phase II task that examines distributions, relationships, and early hypotheses through exploration and simple analyses.","Option B is tempting, but Describe Data is about cataloging structure and fields, not exploring patterns and relationships.", - 
CPMAI-P2-20260226-5012,Phase II — Data Understanding,2,Machine learning model data requirements,10,"A retail startup has only a small labeled dataset for a new recommendation feature. The team needs to define the minimum data needed, decide how to split training/validation/test sets, and identify whether third-party data sources are required. Which Phase II task best covers this work?",Collect Initial Data,Training & Test Data Requirements,Label data,Select Modeling Technique,B,"Training & Test Data Requirements covers minimum training data needs, split strategy, labeling needs, and third-party data considerations in Phase II.","Option A is tempting, but Collect Initial Data is about acquiring datasets; the scenario is about defining training, validation, and test requirements and sufficiency.", - 
CPMAI-P2-20260226-2824,Phase II — Data Understanding,1,Data quality,11,"According to the PMI CPMAI v7 Exam Content Outline, which activity is explicitly part of managing the Data Understanding phase?",Deploy models to production environments,Perform data cleansing and enhancement,Conduct CPMAI Phase IV Go/No-Go assessments,"Validate ""ground truth"" data quality",D,"The Exam Content Outline lists validating ""ground truth"" data quality as a Data Understanding (Phase II) activity.","Option B is tempting, but data cleansing and enhancement is addressed in Data Preparation (Phase III), not Data Understanding.", - 
CPMAI-P2-20260226-1434,Phase II — Data Understanding,2,Collect initial data,12,"A media company is building an AI assistant and has pulled chat transcripts and support tickets. Leadership asks, ""Do we even have the right kind of data for this pattern, and what does it look like at a high level?"" The team needs to document the format, record counts, and whether the data is structured or unstructured. Which Phase II task is the best fit?",Describe Data,Explore Data & Cognitive Data Requirements,Verify Data Quality,Label data,A,"Describe Data is the Phase II task that reports on the acquired data’s format, quantity, fields, and surface characteristics, including structured versus unstructured fit.","Option B is tempting, but exploration goes deeper into distributions and relationships; leadership's question is about high-level description and fit.", - 
CPMAI-P2-20260226-2535,Phase II — Data Understanding,2,Data quality,13,"A city agency is using inspection reports to predict which buildings need follow-up visits. During Phase II, the team wants to understand how often values are missing, how missingness is represented, and where errors show up. Which task should drive that assessment?",Verify Data Quality,Collect Initial Data,Enhance & Augment data,Select data,A,"Verify Data Quality examines completeness, errors, and missing values (including how they are represented and where they occur) and documents the findings.","Option C is tempting, but Enhance & Augment data is a Phase III activity focused on creating derived attributes or new records, not assessing current quality.", - 
CPMAI-P2-20260226-4657,Phase II — Data Understanding,1,Machine learning model data requirements,14,"In Phase II, which task group contains the tasks Training & Test Data Requirements and Edge Model Data Needs?",Collect initial data,Machine learning model data requirements,Data quality,Data Selection,B,Machine learning model data requirements is the Phase II task group that contains both Training & Test Data Requirements and Edge Model Data Needs.,"Option A is tempting because Collect initial data is also a Phase II task group, but it contains Collect Initial Data, Describe Data, and Explore Data tasks — not the model-specific data requirements.", - 
CPMAI-P2-20260226-9935,Phase II — Data Understanding,3,Collect initial data,15,"A team delivers a document listing each dataset, its file format, the number of rows and columns, and the field names. Another stakeholder claims the team already completed ""exploration"" and can skip straight to Phase III. Which Phase II task did the team actually complete?",Explore Data & Cognitive Data Requirements,Describe Data,Collect Initial Data,Verify Data Quality,B,"Describe Data focuses on format, quantity, field identities, and other surface features, which matches the document produced.","TRAP NOTE: Option A is tempting, but exploration requires inspecting distributions, relationships, and forming early findings or hypotheses, not just listing fields and counts.",The discrimination is surface cataloging (Describe Data) versus analytic exploration of patterns and hypotheses (Explore Data & Cognitive Data Requirements).
CPMAI-P2-20260226-4257,Phase II — Data Understanding,1,Data quality,16,Which Phase II activity in the PMI CPMAI v7 Exam Content Outline addresses deciding whether the team should loop back to earlier phases based on what they learn from the data?,Conduct CPMAI Phase III Go/No-Go assessments,Determine when to iterate back to previous phases,Execute CPMAI Phase V Go/No-Go assessments,Implement model iteration processes,B,The Data Understanding phase includes determining when to iterate back to previous phases based on findings and feasibility.,"Option A is tempting, but Phase III Go/No-Go is tied to Data Preparation, while this question asks about the Phase II decision to loop back.", - 
CPMAI-P2-20260226-1409,Phase II — Data Understanding,2,Machine learning model data requirements,17,"An edtech company has enough raw text data, but not enough labeled examples for supervised learning. The team needs to decide what labeling must occur, what third-party sources might help, and what transformations could increase usable training volume without breaking quality. Which Phase II task should they follow?",Select cognitive-relevant algorithm / modeling technique,Label data,Collect Initial Data,Training & Test Data Requirements,D,"Training & Test Data Requirements in Phase II covers minimum data needs, labeling requirements for training data, and third-party source considerations.","Option B is tempting, but Label data is Phase III execution; Phase II here is about defining requirements and feasibility before committing to labeling work.", - 
CPMAI-P2-20260226-7873,Phase II — Data Understanding,3,Collect initial data,18,"A logistics team has a huge dataset and wants to drop columns immediately to reduce storage and speed up analysis. Two choices look reasonable: prune columns now or first analyze what the data contains and what might matter. In CPMAI, what is the best Phase II guidance?",Clean data now so exploration is done on a perfect dataset,Select data now to reduce the dataset before any analysis,Explore Data & Cognitive Data Requirements to understand characteristics first; postpone column/row pruning to Phase III Select data,Select Modeling Technique now to determine which columns will matter,C,Explore Data & Cognitive Data Requirements is the Phase II task for inspecting data characteristics and surfacing what matters before Phase III selection and pruning.,"TRAP NOTE: Option B is tempting, but Select data is explicitly a Phase III Data Preparation task; Phase II exploration informs selection rather than replacing it.",The boundary is Phase II exploration and requirement discovery versus Phase III data selection/pruning decisions.
CPMAI-P2-20260226-4582,Phase II — Data Understanding,2,Data quality,19,"A hospital system merges labels from three clinics and notices that ""positive"" means different things across sites. The model’s early results look unstable, and leaders want to keep moving. In Phase II, which action best aligns to CPMAI guidance before proceeding?",Proceed to Phase IV because model iteration will resolve label inconsistencies,Proceed to Phase III and let cleansing fix the label mismatch,"Validate ""ground truth"" and data quality issues, and decide whether to iterate back based on feasibility",Skip label checks and rely on more data to average out the differences,C,"Phase II includes validating ""ground truth"" and overall data quality and determining whether findings require iteration back to earlier phases.","Option B is tempting, but label meaning conflicts are not solved by generic cleansing; Phase II requires validating ground truth and feasibility before moving on.", - 
CPMAI-P2-20260226-9279,Phase II — Data Understanding,1,Machine learning model data requirements,20,"In Phase II, what is the relationship between the Describe Data task and the Explore Data & Cognitive Data Requirements task?","Describe Data catalogs surface characteristics (format, fields, volume); Explore Data examines distributions, relationships, and patterns",Describe Data and Explore Data are interchangeable names for the same task,Explore Data must be completed before Describe Data can begin,Describe Data is a Phase III task that follows Phase II exploration,A,"Describe Data produces a surface-level catalog of data characteristics (format, quantity, fields), while Explore Data & Cognitive Data Requirements goes deeper into distributions, relationships, and early hypotheses. Both are distinct Phase II tasks under Collect initial data.","Option B is the strongest trap — candidates may assume description and exploration are the same activity. However, CPMAI distinguishes cataloging structure (Describe Data) from analytic exploration of patterns (Explore Data & Cognitive Data Requirements).", - 
CPMAI-P3-20250226-7184,Phase III - Data Preparation,1,Clean and Transform Data,1,"Which Phase III task involves handling missing values, removing duplicates, and correcting inconsistent formats in the dataset?",Clean and Transform Data,Split Datasets,Perform Feature Engineering,Document Data Preparation Activities,A,"Clean and Transform Data is the Phase III task responsible for all data cleaning activities including missing value treatment, deduplication, and format standardization.","Option C (Feature Engineering) is tempting because it also modifies data, but feature engineering creates new attributes while cleaning fixes existing data issues.", - 
CPMAI-P3-20250226-3529,Phase III - Data Preparation,2,Perform Feature Engineering,2,"A telecommunications company is preparing data for a customer churn prediction model. The data scientist creates a new feature called ""avg_monthly_spend"" by dividing total annual revenue by 12 and another feature called ""support_calls_per_month"" by dividing total support calls by account age in months. Which Phase III activity is being performed?",Data Cleaning,Data Normalization,Feature Engineering,Dataset Splitting,C,"Creating new features from existing data (avg_monthly_spend, support_calls_per_month) is the core activity of Perform Feature Engineering.","Option B (Data Normalization) is tempting because it involves mathematical transformation, but normalization scales existing features rather than creating new derived attributes.", - 
CPMAI-P3-20250226-9641,Phase III - Data Preparation,3,Split Datasets,3,"A project team has prepared a dataset for a fraud detection model. The data scientist creates three subsets: 60% for training, 20% for validation during model tuning, and 20% held back for final testing. According to CPMAI, what is the most accurate statement about this splitting approach?",The validation set should be combined with the test set to ensure sufficient data for final evaluation,The split should be 70/30 training/test only - validation is performed during cross-validation,This three-way split is correct for Phase III to support both model tuning and unbiased final evaluation,"The test set should be created in Phase V during model evaluation, not in Phase III",C,"Creating separate training, validation, and test sets during Phase III is correct - validation supports iterative tuning while the held-out test set provides unbiased final evaluation.","Option D is tempting because testing occurs in Phase V, but the test dataset must be created during Phase III preparation to prevent data leakage.",The distinction between when datasets are created (Phase III) versus when they are used (Phase IV/V) is critical - test sets are prepared in Phase III but not used until Phase V evaluation.
CPMAI-P3-20250226-5278,Phase III - Data Preparation,1,Document Data Preparation Activities,4,"Which Phase III document typically contains details about transformation steps applied, features created, and the rationale for data cleaning decisions?",Data Quality Report,Data Preparation Summary,Project Charter,Model Card,B,"The Data Preparation Summary (or equivalent documentation) captures all transformation steps, feature engineering decisions, and cleaning rationale for handoff to Phase IV.","Option A (Data Quality Report) is tempting because it is a key document, but it is created in Phase II, not Phase III.", - 
CPMAI-P3-20250226-1935,Phase III - Data Preparation,2,Clean and Transform Data,5,"A healthcare analytics project has patient age data that ranges from 0 to 120 years and income data from $10,000 to $500,000. The data scientist decides to standardize both features using z-scores but does not document why standardization was chosen over min-max normalization. The PM flags this during a Phase III review. According to CPMAI, what is the primary issue?",The transformation choice must be documented with rationale as part of Phase III governance,Standardization is the wrong technique for this data — min-max should have been used,Transformations should be deferred to Phase IV so the algorithm can determine the approach,The data scientist should have consulted the Phase II Data Quality Report before transforming,A,CPMAI requires that all Phase III data preparation decisions — including transformation choices — be documented with rationale. The issue is not the technique itself but the lack of documentation and justification.,"Option B is tempting because technique selection matters, but CPMAI does not prescribe specific methods — the framework requires governance and documentation of the choice made.", - 
CPMAI-P3-20250226-7406,Phase III - Data Preparation,3,Perform Feature Engineering,6,"A manufacturing predictive maintenance project has sensor readings recorded every second. The data scientist creates features including rolling 5-minute averages, max/min values per hour, and rate of change between readings. The project sponsor asks why these features weren't created earlier in Phase II during data exploration. What should the data scientist explain?",Feature engineering deliberately occurs in Phase III after data understanding is complete and before modeling begins,These features could have been created in Phase II but were overlooked,These features should be created in Phase IV during model training based on algorithm requirements,Feature engineering is an iterative process that spans Phases II through IV,A,Feature engineering is a Phase III activity that transforms prepared data into modeling inputs - it builds on Phase II understanding but is executed after cleaning and before modeling.,"Option D is tempting because iteration is a CPMAI principle, but feature engineering is specifically allocated to Phase III, not distributed across phases.",The boundary between Phase II exploration and Phase III feature creation is critical - exploration identifies patterns; feature engineering creates modeling inputs from those insights.
CPMAI-P3-20250226-4820,Phase III - Data Preparation,1,Split Datasets,7,What is the primary purpose of creating a separate validation set during Phase III dataset splitting?,To evaluate the final model's business impact,To replace the need for cross-validation,To test the model on completely unseen data,To tune model hyperparameters during development,D,The validation set is used during Phase IV model development for hyperparameter tuning and iterative improvement without contaminating the final test set.,"Option C (unseen data testing) describes the test set purpose, not the validation set purpose.", - 
CPMAI-P3-20250226-6153,Phase III - Data Preparation,2,Document Data Preparation Activities,8,"A project team completes data preparation and is preparing for the Phase III gate review. The data scientist wants to document that they removed 5% of records as outliers and applied log transformation to highly skewed features. According to CPMAI, where should this information be recorded?",In the Phase II Data Quality Report as an addendum,Only in the code comments for future reference,In the Data Preparation Summary for handoff to Phase IV,In the project closure documentation,C,"All data preparation decisions, including outlier removal and transformations, must be documented in the Data Preparation Summary to inform Phase IV modeling activities.","Option A is tempting because quality issues were identified in Phase II, but Phase III execution decisions belong in Phase III documentation.", - 
CPMAI-P3-20250226-2974,Phase III - Data Preparation,1,Clean and Transform Data,9,"According to CPMAI, which Phase III task is responsible for handling missing values, correcting errors, and applying transformations to prepare data for modeling?",Clean and Transform Data,Perform Feature Engineering,Verify Data Quality,Document Data Preparation Activities,A,"Clean and Transform Data is the Phase III task that handles missing value treatment, error correction, and data transformations as preparation for modeling.","Option C is tempting because quality verification sounds related to fixing issues, but Verify Data Quality is a Phase II assessment task, not a Phase III remediation task.", - 
CPMAI-P3-20250226-8602,Phase III - Data Preparation,3,Clean and Transform Data,10,"A credit risk modeling project has a feature ""loan_amount"" with values ranging from $1,000 to $50,000. The data scientist applies min-max normalization to scale all values between 0 and 1. Another team member argues this transformation should have been done in Phase II during data exploration. Which statement is most accurate?",The team member is correct — scaling should occur during exploration to understand distributions,Scaling should wait until Phase IV so the algorithm can determine optimal scaling,Both approaches are acceptable as long as documentation is maintained,The team member is incorrect — scaling during Phase II would have distorted exploration statistics,D,Min-max normalization is a data transformation applied in Phase III. Applying it during Phase II would alter the raw data and potentially mislead exploration findings about original distributions.,"Option C is tempting because documentation is important in CPMAI, but the framework reserves transformations for Phase III to maintain data integrity during exploration.",The principle of keeping exploration data in its original form versus transforming for modeling is a key Phase II/III boundary — exploration examines raw data; preparation transforms it.
CPMAI-P3-20250226-5137,Phase III - Data Preparation,2,Perform Feature Engineering,11,"An e-commerce recommendation system project has transaction data with customer_id, product_id, purchase_date, and purchase_amount. The data scientist creates features including ""customer_lifetime_value"" (sum of all purchases), ""days_since_last_purchase"", and ""avg_purchase_amount"". Which type of feature engineering is being performed?",Encoding features,Aggregation features,Normalization features,Text extraction features,B,"Aggregation features summarize multiple records into single customer-level metrics (sum, recency, average) — exactly what the scenario describes.","Option A (Encoding) is tempting because it is a common feature engineering technique, but encoding converts categories to numbers; it does not aggregate transaction history.", - 
CPMAI-P3-20250226-7489,Phase III - Data Preparation,1,Split Datasets,12,"In a classification problem with imbalanced classes (95% non-fraud, 5% fraud), which dataset splitting consideration becomes most important during Phase III?",Ensuring all three datasets have the same number of records,"Maintaining the same class distribution across training, validation, and test sets",Making the test set larger than the training set,Removing the minority class from the validation set,B,"Stratified splitting preserves the original class distribution across all datasets, which is critical for imbalanced problems to ensure each set represents the problem reality.","Option D is tempting because minority classes are small, but removing them from validation would prevent proper tuning for fraud detection.", - 
CPMAI-P3-20250226-3605,Phase III - Data Preparation,2,Document Data Preparation Activities,13,"A project manager is reviewing Phase III documentation before the gate review. She notices the documentation includes feature definitions, transformation formulas, and the random seed used for dataset splitting. According to CPMAI, why is documenting the random seed important?",It speeds up the model training process,It enables reproducible dataset splits for future iterations or audits,It is required for regulatory compliance in all industries,It ensures the test set is larger than the training set,B,"Documenting the random seed ensures that the exact same data splits can be reproduced if needed for model retraining, debugging, or audit purposes.","Option C is tempting because reproducibility is important for compliance, but it is not universally required — the primary purpose is reproducibility for project consistency.", - 
CPMAI-P3-20250226-9261,Phase III - Data Preparation,3,Clean and Transform Data,14,"A customer segmentation project has a feature ""annual_income"" with 10% missing values. The data scientist proposes three options: delete all records with missing income, replace missing values with the median income, or create a separate ""income_unknown"" category. The sponsor asks which approach CPMAI recommends. What should the project manager explain?",CPMAI requires deletion of records with missing values to maintain data integrity,CPMAI specifies median imputation as the standard approach for continuous variables,CPMAI requires creating a separate category for all missing values to preserve sample size,CPMAI does not prescribe a single method — the choice must be documented with rationale based on business context and impact,D,"CPMAI emphasizes documentation and justification of data preparation decisions, not prescribing specific technical methods — the choice depends on business context and should be documented.","Option B is tempting because median imputation is common, but CPMAI does not mandate any specific technique — the framework focuses on governance and documentation.","The distinction between prescribed methods versus governed decision-making is critical — CPMAI provides structure for decisions, not technical prescriptions."
CPMAI-P3-20250226-1847,Phase III - Data Preparation,1,Perform Feature Engineering,15,"In Phase III, what is the relationship between Clean and Transform Data and Perform Feature Engineering?",They are the same task with different names,Feature engineering must be completed before any cleaning can begin,Cleaning occurs in Phase II; only feature engineering belongs in Phase III,Feature engineering creates new derived attributes from cleaned data; cleaning fixes existing data issues,D,"Clean and Transform Data fixes existing data issues (missing values, errors, formats), while Perform Feature Engineering creates new derived attributes that serve as modeling inputs. Both are distinct Phase III tasks.","Option C is tempting because Phase II includes data quality assessment, but the actual cleaning and remediation work is a Phase III activity, not Phase II.", - 
CPMAI-P3-20250226-6392,Phase III - Data Preparation,2,Split Datasets,16,"A project team creates training, validation, and test sets during Phase III. During Phase IV model development, the data scientist uses the validation set repeatedly for hyperparameter tuning. After tuning is complete, the model achieves 92% accuracy on the validation set. What is the most important limitation of this accuracy estimate?",The limitation is that validation accuracy cannot be calculated for classification problems,It underestimates true performance because validation sets are smaller than training sets,It is accurate because validation sets are specifically designed for performance estimation,It overestimates true performance because the model has been indirectly fitted to the validation set,D,"Repeated use of the validation set for tuning leads to indirect overfitting - the validation accuracy becomes optimistic, which is why the held-out test set is needed for final evaluation.","Option C is tempting because validation sets are used for performance monitoring during tuning, but they are not unbiased estimates of real-world performance.", - 
CPMAI-P3-20250226-4753,Phase III - Data Preparation,2,Clean and Transform Data,17,"A logistics optimization project has location data with longitude values ranging from -180 to 180 and distance values ranging from 0 to 10,000 kilometers. The data scientist applies scaling to both features so they contribute equally to distance calculations. Which transformation is most appropriate?",One-hot encoding of both features,Min-max normalization to a 0-1 range for both features,Removing the longitude feature entirely,Applying different transformations to each feature based on distribution,B,Min-max normalization to a common range (typically 0-1) ensures features with different scales contribute equally to distance-based algorithms like k-means or k-NN.,"Option D is tempting because transformations should consider distributions, but the scenario specifically requires equal contribution to distance calculations, which demands common scaling.", - 
CPMAI-P3-20250226-8015,Phase III - Data Preparation,3,Perform Feature Engineering,18,"A natural language processing project for customer feedback analysis has 50,000 text comments. The data scientist creates features including word counts, sentiment scores, and TF-IDF vectors. A team member suggests these features should have been created in Phase II during text exploration. What is the most accurate CPMAI-based response?","Phase II text exploration would examine sample comments and identify themes, but feature engineering for modeling occurs in Phase III",Text features can be created in either phase as long as they are documented,Phase II should include complete feature engineering because text requires more preprocessing,Feature engineering for text should wait until Phase IV so the model can guide feature selection,A,Phase II explores raw data to understand content and quality; Phase III transforms that understanding into engineered features suitable for modeling algorithms.,"Option C is tempting because text requires significant preprocessing, but exploration (Phase II) and feature engineering (Phase III) remain distinct activities.",The Phase II/III boundary for unstructured data mirrors structured data - exploration examines raw content; preparation creates modeling features.
CPMAI-P3-20250226-2936,Phase III - Data Preparation,1,Document Data Preparation Activities,19,Which of the following is a primary output of Phase III that serves as the direct input to Phase IV Model Development?,Business Problem Statement,Data Quality Report,Model-Ready Datasets,Model Performance Metrics,C,"Model-Ready Datasets (training, validation, test) are the primary Phase III outputs that feed directly into Phase IV modeling activities.","Option B (Data Quality Report) is tempting because it is a key deliverable, but it is a Phase II output, not the primary handoff to Phase IV.", - 
CPMAI-P3-20250226-5608,Phase III - Data Preparation,2,Clean and Transform Data,20,"A retail analytics project has customer transaction data with 2% duplicate records. Some duplicates are exact copies, while others have identical transaction IDs but slight variations in timestamp or amount. According to CPMAI, how should the project team handle this during Phase III?",Keep all records since 2% duplication is below the 5% quality threshold,Manually review each duplicate pair to determine which to keep,Investigate root causes of duplication and develop systematic deduplication rules,Ignore the issue since duplicates don't affect most algorithms,C,Systematic investigation of duplication root causes and rule-based deduplication is the correct Phase III approach - addressing symptoms without understanding causes leads to recurring issues.,"Option B is tempting because manual review seems thorough, but it is not scalable and does not address systematic causes — CPMAI emphasizes process over one-off fixes.", - 
CPMAI-P3-20260226-7391,Phase III - Data Preparation,2,Data Selection,1,"A streaming company has 18 months of clickstream logs, but compute costs are exploding and many fields are irrelevant to the churn target. The team needs to choose only the rows and columns needed for modeling and document what was excluded. Which Phase III task should they use?",Verify Data Quality,Clean data,Explore Data & Cognitive Data Requirements,Select data,D,Select data is the Phase III task that decides which records and attributes will be used and documents inclusion and exclusion rationale.,"Option B is tempting, but Clean data is about improving quality of already selected data rather than choosing which rows and columns belong in the dataset.", - 
CPMAI-P3-20260226-1842,Phase III - Data Preparation,1,Data Cleansing & Enhancement,2,"In Phase III, which task takes the dataset chosen by Select data and brings it to modeling-ready quality by addressing missing values, formatting inconsistencies, and errors?",Clean data,Select data,Enhance & Augment data,Label data,A,"Clean data focuses on raising selected data quality to modeling-ready levels through cleansing, defaults, missing-data handling, and formatting.","Option B is tempting, but Select data chooses rows and columns to include or exclude rather than fixing quality problems in the chosen data.", - 
CPMAI-P3-20260226-9065,Phase III - Data Preparation,2,Data Labeling,3,"A robotics team has thousands of warehouse images but no annotations for object locations. They need to define labeling needs, pick a labeling method, estimate cost, and verify label quality before model building starts. Which Phase III task applies?",Training & Test Data Requirements,Label data,Clean data,Model Training / Model Building,B,"Label data covers identifying labeling needs and methods, executing labeling, and defining how label quality will be verified.","Option A is tempting, but Training & Test Data Requirements is a Phase II task about defining needs, while Phase III Label data executes labeling and documents the method and quality checks.", - 
CPMAI-P3-20260226-3318,Phase III - Data Preparation,3,Data Selection,4,"A sponsor says, “Just remove duplicates and we’ll be fine.” The dataset is also bloated with irrelevant columns and historical rows outside the modeling scope, and the team must document what gets excluded to manage volume and complexity. Which Phase III task is the best match?",Enhance & Augment data,Clean data,Select data,Label data,C,Select data explicitly covers selecting both attributes (columns) and records (rows) and documenting inclusion and exclusion decisions.,"TRAP NOTE: Option B is tempting, but Clean data fixes quality issues in selected data while Select data is the task for choosing what data to include and documenting exclusions.",The boundary is selection and documentation of included or excluded rows and columns (Select data) versus quality remediation of the chosen dataset (Clean data).
CPMAI-P3-20260226-5704,Phase III - Data Preparation,1,Data Selection,5,"Which Phase III task is responsible for deciding both which columns and which rows will be included in the modeling dataset, and documenting what was excluded?",Explore Data & Cognitive Data Requirements,Clean data,Enhance & Augment data,Select data,D,Select data covers choosing both attributes (columns) and records (rows) for the modeling dataset and documenting inclusion and exclusion decisions.,"Option A is tempting because exploration examines data characteristics, but exploration is a Phase II task - the actual selection decision is Phase III.", - 
CPMAI-P3-20260226-8426,Phase III - Data Preparation,2,Data Cleansing & Enhancement,6,"An insurance team finds inconsistent date formats, impossible ages, and missing values after selecting the dataset. They need to bring the data to modeling-ready quality using defaults, imputation, and formatting fixes. Which Phase III task should they execute?",Clean data,Enhance & Augment data,Verify Data Quality,Select cognitive-relevant algorithm / modeling technique,A,Clean data is the Phase III task that brings selected data quality to the level required for modeling using cleansing and missing-data handling.,"Option C is tempting, but Verify Data Quality is Phase II assessment, while Phase III Clean data performs the remediation actions.", - 
CPMAI-P3-20260226-1967,Phase III - Data Preparation,1,Data Labeling,7,Which item is explicitly included in the Phase III Label data task artifacts?,Decide which algorithm family to use for supervised learning,"Identify a labeling method, including internal labor, third-party labor, pre-existing labeled data, or mixed modes","Define training, validation, and test split strategy",Measure business KPI impact of the model,B,Label data includes selecting a labeling method and documenting how labeling will be done and validated.,"Option C is tempting, but training and test split strategy is defined in Phase II Training & Test Data Requirements, not in Phase III Label data.", - 
CPMAI-P3-20260226-6552,Phase III - Data Preparation,2,Data Cleansing & Enhancement,8,"A public-sector team wants a repeatable way to ingest raw data, prepare training data, and then run the same preparation steps on live inference data after launch. Which Phase III task explicitly calls for creating reusable training and inference data pipelines as artifacts?",Model Training / Model Building,Select data,Enhance & Augment data,Clean data,D,Clean data includes creating reusable data pipelines for training and for real-world inference as part of preparing data for modeling.,"Option C is tempting, but Enhance & Augment data focuses on derived attributes and new records, not the baseline reusable cleansing pipelines for training and inference.", - 
CPMAI-P3-20260226-4209,Phase III - Data Preparation,1,Data Cleansing & Enhancement,9,"In Phase III, which task is responsible for creating derived features, generating synthetic records, and producing transformed values to strengthen the modeling dataset?",Enhance & Augment data,Clean data,Select data,Label data,A,"Enhance & Augment data covers creating derived features, new records, or transformed values to strengthen the modeling dataset.","Option B is tempting, but Clean data is focused on fixing quality issues (missingness, formatting, errors) rather than creating new attributes or records.", - 
CPMAI-P3-20260226-8735,Phase III - Data Preparation,2,Data Cleansing & Enhancement,10,"A national retailer wants to predict weekly store demand for inventory optimization. The data team has transaction history and store metadata, but leadership wants richer signals - like sales-per-square-foot and local weather correlations. The team proposes merging store characteristics with external weather history and computing derived metrics. Which Phase III task best fits this work?",Clean data,Enhance & Augment data,Select data,Label data,B,Enhance & Augment data is the Phase III task for creating derived attributes and enhancing data by combining information from multiple sources.,"Option A is tempting, but Clean data corrects data quality problems and formatting; it does not cover constructing new features or merged enrichment records.", - 
CPMAI-P3-20260226-2441,Phase III - Data Preparation,3,Data Cleansing & Enhancement,11,"A team argues that joining two tables is “just cleaning,” since it makes the dataset easier to use. The real goal is to create new, richer attributes by combining multiple sources into new values for modeling. Which Phase III task is the most accurate match?",Select data,Clean data,Enhance & Augment data,Verify Data Quality,C,Enhance & Augment data covers combining information from multiple tables or records to create new records or values for modeling.,"TRAP NOTE: Option B is tempting, but Clean data addresses quality problems like missingness and formatting, while enrichment joins to create new values are augmentation.",The distinction is constructive feature or record creation by combining sources (Enhance & Augment data) versus quality remediation and formatting of selected data (Clean data).
CPMAI-P3-20260226-9186,Phase III - Data Preparation,1,Data Labeling,12,"Which Phase III task group includes defining labeling methods, executing annotation work, estimating labeling costs, and verifying label quality?",Data Cleansing & Enhancement,Data Selection,Data Labeling,Machine learning model data requirements,C,Data Labeling is the Phase III task group dedicated to adding labels and annotations needed for supervised learning workflows.,"Option A is tempting, but Data Cleansing & Enhancement focuses on quality fixes and augmentation, not labeling or annotation.", - 
CPMAI-P3-20260226-5077,Phase III - Data Preparation,3,Data Selection,13,A team claims that dropping columns is part of Phase II exploration because “we learned they aren’t useful.” Another stakeholder says it belongs in Phase III because it changes what will be used for modeling and must be documented for future reference. Which task is the correct match?,Explore Data & Cognitive Data Requirements,Select data,Verify Data Quality,Clean data,B,Select data is responsible for choosing which attributes and records will be used for modeling and documenting inclusion and exclusion decisions.,"TRAP NOTE: Option A is tempting, but Phase II exploration studies characteristics and hypotheses, while Phase III Select data makes and documents the inclusion or exclusion decision that defines the modeling dataset.",The boundary is Phase II analysis and exploration versus Phase III dataset definition through documented row and column selection (Select data).
CPMAI-P3-20260226-3014,Phase III - Data Preparation,1,Data Labeling,14,"Within Phase III Label data, what is an explicit requirement related to label quality?",Identify a method for verifying the quality of data labeling,Use k-fold cross-validation to validate labels,Tune hyperparameters to reduce label noise,Skip label checks if a pre-trained model is used,A,Label data requires defining how labeling quality will be verified as part of the labeling method and outcomes.,"Option B is tempting, but k-fold cross-validation is a model evaluation technique and not the mechanism for verifying annotation accuracy in the Label data task.", - 
CPMAI-P3-20260226-6679,Phase III - Data Preparation,2,Data Labeling,15,"A startup will use contractors to label audio clips for supervised learning, but leadership worries the label quality will drift over time as volume ramps. The team needs a plan for quality checks, costs, and how the approach scales. Which Phase III task should they execute?",Enhance & Augment data,Clean data,Label data,Training & Test Data Requirements,C,"Label data includes selecting a labeling method, estimating costs, defining label-quality verification, and reporting labeling outcomes as needs scale.","Option D is tempting, but Phase II Training & Test Data Requirements defines needs, while Phase III Label data covers execution details, costs, and verification.", - 
CPMAI-P3-20260226-9902,Phase III - Data Preparation,3,Data Labeling,16,"A sponsor wants to begin training a supervised model immediately because “we can label later.” The dataset is selected and cleaned, but labels and annotations do not yet exist. Under CPMAI, what is the correct next task?",Verify Data Quality,Model Training / Model Building,Select cognitive-relevant algorithm / modeling technique,Label data,D,Label data must be completed to add required labels and annotations before Phase IV model training for supervised learning can proceed.,"TRAP NOTE: Option B is tempting, but Model Training / Model Building is Phase IV and depends on having the required labeled dataset produced by Phase III Label data.",The boundary is Phase III creation of labels and annotations (Label data) versus Phase IV training the model on the prepared dataset (Model Training / Model Building).
CPMAI-P3-20260226-4128,Phase III - Data Preparation,1,Data Selection,17,Which set of criteria is explicitly referenced by the Phase III Select data task when deciding what data will be used for analysis?,"Relevance to the goals, data quality, and technical constraints such as limits on volume or data types",Only business ROI targets and stakeholder preferences,Only what is easiest to collect and label quickly,Only model accuracy results from prior training runs,A,"Select data bases inclusion on relevance, quality, and technical constraints such as volume and data types.","Option D is tempting, but model accuracy results require a trained model, which occurs after Phase III and therefore cannot drive initial data selection.", - 
CPMAI-P3-20260226-7750,Phase III - Data Preparation,2,Data Cleansing & Enhancement,18,A bank completed Phase II Verify Data Quality and documented missing fields and inconsistent formats. Now they must apply cleansing actions and document what decisions and transformations were taken to address those reported problems. Which Phase III task should they perform?,Explore Data & Cognitive Data Requirements,Select data,Enhance & Augment data,Clean data,D,Clean data includes performing cleansing operations and documenting the decisions and actions taken to address quality problems identified in Phase II.,"Option A is tempting, but exploration happens in Phase II and does not include executing or documenting remediation actions taken to fix reported quality issues.", - 
CPMAI-P3-20260226-2683,Phase III - Data Preparation,2,Data Selection,19,"A healthcare project has multiple candidate sources, but privacy rules and storage limits mean only a subset can be used, and the team must record what was excluded and why for auditability. Which Phase III task is the best fit?",Clean data,Select data,Enhance & Augment data,Label data,B,Select data is used to choose the data to be used for analysis under quality and technical constraints and to document inclusion and exclusion rationale.,"Option A is tempting, but Clean data improves the quality of already selected data and does not focus on documenting excluded sources and selection rationale.", - 
CPMAI-P3-20260226-5496,Phase III - Data Preparation,1,Data Selection,20,"In the CPMAI lifecycle, what is the relationship between Phase II Verify Data Quality and Phase III Clean data?",Phase III Clean data replaces the need for Phase II quality verification,Both tasks perform the same work but are documented separately,Phase II verifies and documents quality issues; Phase III performs the remediation actions,Phase II performs the cleaning and Phase III only documents what was done,C,"Phase II Verify Data Quality assesses and documents quality problems. Phase III Clean data takes those documented findings and performs the actual remediation - fixing missing values, formatting, and errors. They are sequential and complementary.",Option D is tempting because it reverses the correct sequence. Candidates who confuse which phase assesses versus which phase remediates may select this option., - 
CPMAI-P4-20250226-4721,Phase IV - Model Development,1,Select Modeling Technique,1,"Which Phase IV task involves evaluating and choosing the appropriate algorithm based on the problem type, data characteristics, and business requirements?",Select Modeling Technique,Perform Model Tuning and Validation,Build and Train Model,Document Model Development Activities,A,"Select Modeling Technique is the Phase IV task responsible for evaluating and choosing algorithms based on problem type, data characteristics, and business requirements before training begins.","Option C (Build and Train Model) is tempting because it follows selection, but selection is a distinct preceding task in Phase IV.", - 
CPMAI-P4-20250226-8392,Phase IV - Model Development,2,Build and Train Model,2,A project team has prepared a customer churn dataset and selected logistic regression as the algorithm. The data scientist runs the training algorithm on the training dataset and records the resulting model coefficients. Which Phase IV activity is being performed?,Select Modeling Technique,Perform Model Tuning and Validation,Build and Train Model,Document Model Development Activities,C,Running the training algorithm on the prepared dataset to create a model with learned parameters is the core activity of Build and Train Model.,"Option B (Model Tuning and Validation) is tempting because it follows initial training, but tuning is a separate optimization step after the initial model is built.", - 
CPMAI-P4-20250226-1537,Phase IV - Model Development,1,Perform Model Tuning and Validation,3,"According to CPMAI, what is the primary purpose of the Perform Model Tuning and Validation task in Phase IV?",To select which algorithm family to use for the project,To collect additional training data to improve model accuracy,To deploy the tuned model into the production environment,To optimize model hyperparameters and validate performance before Phase V evaluation,D,Perform Model Tuning and Validation is the Phase IV task that optimizes model configuration through hyperparameter adjustment and validates performance using validation data before the model proceeds to Phase V evaluation.,"Option ? is tempting because algorithm selection is a Phase IV activity, but it belongs to Select Modeling Technique, not the tuning and validation task.", - 
CPMAI-P4-20250226-6294,Phase IV - Model Development,2,Document Model Development Activities,4,"A project team completes initial model training and tuning. Before moving to Phase V, they create documentation containing model architecture diagrams, hyperparameter values, and training performance metrics. What is the primary purpose of this documentation?",To satisfy regulatory requirements only,To replace the need for a separate Phase V evaluation,To provide input to Phase V Model Evaluation and enable reproducibility,To serve as the final project archive,C,"Document Model Development Activities captures all model specifications, parameters, and performance data specifically to inform Phase V evaluation and ensure reproducibility.","Option A is tempting because compliance is important, but the primary purpose is handoff to evaluation and reproducibility, not compliance alone.", - 
CPMAI-P4-20250226-7483,Phase IV - Model Development,1,Select Modeling Technique,5,Which factor is most important when selecting between a highly interpretable model (like linear regression) and a complex black-box model (like deep learning) during Phase IV?,The computational cost of training,The size of the training dataset,The business requirement for explainability,The programming language preference,C,Business requirements for explainability and transparency are critical factors in algorithm selection - some use cases require interpretable models regardless of performance advantages of black-box approaches.,"Option B (dataset size) is important but secondary to business requirements — if explainability is mandated, even large datasets don't justify opaque models.", - 
CPMAI-P4-20250226-3916,Phase IV - Model Development,3,Build and Train Model,6,A project manager oversees a computer vision project for manufacturing quality control. The data scientist proposes using a pre-trained ResNet model and fine-tuning it on the company's defect images rather than training a model from scratch. The sponsor asks why this approach is being taken. What should the project manager explain?,Fine-tuning a pre-trained model leverages transfer learning to reduce data requirements and training time while achieving strong performance,Building from scratch would violate licensing agreements,Pre-trained models are required by CPMAI for all computer vision projects,Fine-tuning is only appropriate when the pre-trained model was trained on identical data,A,"Using pre-trained models with fine-tuning (transfer learning) is a Phase IV best practice that leverages existing learned features, reducing data needs and training time.","Option B is tempting because licensing might be a consideration, but the primary reason is technical efficiency through transfer learning, not legal constraints.","The distinction between training from scratch versus transfer learning is a key Phase IV concept - transfer learning is appropriate when the pre-trained model's domain is relevant, not just for licensing reasons."
CPMAI-P4-20250226-5270,Phase IV - Model Development,2,Perform Model Tuning and Validation,7,"A data scientist is tuning a gradient boosting model for a fraud detection system. She tests 40 different hyperparameter combinations over two weeks, keeping detailed logs of each run. The project manager asks why she is logging failed experiments rather than just the final configuration. According to CPMAI, what is the best justification for this practice?",Failed experiments should not be logged — only the final configuration matters,Logging is only necessary if the project will be handed off to a different team,Experiment logs are only required for regulated industries,Logging all experiments creates an audit trail that supports reproducibility and informs future iterations,D,"CPMAI requires documenting the full exploration path during Phase IV tuning, including failed experiments. This supports reproducibility, enables audit, and creates organizational learning for future model development iterations.","Option A is tempting because teams often focus only on final results, but CPMAI emphasizes documenting the full tuning journey as part of Phase IV governance.", - 
CPMAI-P4-20250226-2643,Phase IV - Model Development,2,Document Model Development Activities,8,"A project team completes Phase IV and prepares for the gate review. The documentation includes the final hyperparameter values used, but the sponsor asks why it also includes records of all hyperparameter combinations attempted during tuning. According to CPMAI, why is this important?",It is not important and adds unnecessary documentation overhead,It provides an audit trail of the tuning process and supports reproducibility,It ensures the project meets minimum documentation page count requirements,It allows the team to revert to earlier models if the final model fails,B,"Documenting attempted hyperparameter combinations creates an audit trail of the tuning process, supports reproducibility, and provides valuable learning for future projects.","Option D is tempting because version control is important, but the primary purpose of documenting tuning attempts is the audit trail and organizational learning.", - 
CPMAI-P4-20250226-9851,Phase IV - Model Development,1,Build and Train Model,9,Which Phase IV task takes the selected algorithm and prepared data as inputs and produces a trained model with learned parameters as output?,Select Modeling Technique,Perform Model Tuning and Validation,Document Model Development Activities,Build and Train Model,D,Build and Train Model is the Phase IV task that executes the selected algorithm on the prepared data to produce a trained model with learned parameters.,"Option A is tempting because technique selection precedes training, but Select Modeling Technique chooses the algorithm — Build and Train Model executes it.", - 
CPMAI-P4-20250226-4372,Phase IV - Model Development,3,Select Modeling Technique,10,"A healthcare diagnostics project requires a model that can predict disease risk. The business stakeholders mandate that the model must provide explanations for each prediction to comply with medical regulations. The data science team is considering three options: logistic regression, random forest, and a neural network. According to CPMAI, what is the most important consideration?",Select the neural network because it typically has highest accuracy,Select logistic regression because it is most interpretable and meets the explainability requirement,Select random forest as a compromise between accuracy and interpretability,Run all three and let Phase V evaluation determine the choice,B,"When business requirements mandate explainability, algorithm selection must prioritize interpretability - logistic regression provides clear explanations of feature contributions, meeting the regulatory need.","Option C is tempting because random forest offers some interpretability, but if full explainability is mandated, logistic regression's complete transparency is required.",The trade-off between accuracy and interpretability is a key Phase IV decision point - business requirements (explainability) can override pure accuracy considerations.
CPMAI-P4-20250226-7401,Phase IV - Model Development,2,Perform Model Tuning and Validation,11,"A data scientist completes initial training of a k-nearest neighbors model and wants to estimate how well it will perform on unseen data. She divides the training data into 5 folds, trains on 4 folds, and evaluates on the held-out fold, repeating this process 5 times. Which validation technique is being used?",Holdout validation,Bootstrapping,Hyperparameter tuning,K-fold cross-validation,D,"K-fold cross-validation partitions data into k folds, trains on k-1 folds, and evaluates on the held-out fold, repeating k times to get a robust performance estimate.","Option A (Holdout validation) is tempting because it is a common validation approach, but holdout uses only a single split and provides less reliable estimates than k-fold.", - 
CPMAI-P4-20250226-6138,Phase IV - Model Development,1,Document Model Development Activities,12,"Which Phase IV document typically contains the final model architecture, hyperparameter values, training performance metrics, and version control information?",Model Development Summary,Project Charter,Data Dictionary,Business Requirements Document,A,The Model Development Summary (or equivalent) captures all technical details of the developed model for handoff to Phase V evaluation.,"Option C (Data Dictionary) is a Phase II document, not the Phase IV Model Development Summary.", - 
CPMAI-P4-20250226-2947,Phase IV - Model Development,3,Build and Train Model,13,A project team is developing a fraud detection model. The data scientist proposes using AutoML tools to automatically try multiple algorithms and hyperparameter combinations. The sponsor is concerned this violates CPMAI's emphasis on deliberate decision-making. What should the project manager explain?,AutoML is acceptable in Phase IV as it accelerates model development while still requiring oversight and documentation,The sponsor is correct - AutoML should only be used in Phase III,AutoML replaces the need for human involvement in model development,AutoML is only appropriate for non-production proof-of-concept projects,A,"AutoML tools are valuable in Phase IV for accelerating algorithm selection and hyperparameter tuning, but they require human oversight and documentation of results - they augment, not replace, deliberate decision-making.","Option B is tempting because AutoML seems too automated, but CPMAI embraces appropriate automation while requiring human governance over the process.","The distinction between automation (acceptable with governance) versus abdication (not acceptable) is critical - AutoML is a tool, not a replacement for project management oversight."
CPMAI-P4-20250226-8560,Phase IV - Model Development,1,Select Modeling Technique,14,"According to CPMAI, what must be documented as part of the Select Modeling Technique task in Phase IV?",Only the name of the chosen algorithm,Only the final model accuracy on the test set,"The rationale for choosing the algorithm, including why alternatives were rejected",The production deployment plan for the selected model,C,"Select Modeling Technique requires documenting not just the chosen algorithm but the rationale for the selection, including why alternative algorithms were considered and rejected. This supports reproducibility and governance.","Option A is tempting because naming the algorithm seems sufficient, but CPMAI requires the full selection rationale including alternatives considered.", - 
CPMAI-P4-20250226-1024,Phase IV - Model Development,2,Perform Model Tuning and Validation,15,"A data scientist tunes a support vector machine by trying different kernel functions and regularization parameters. After each combination, she evaluates performance on the validation set. After 30 combinations, she selects the one with highest validation accuracy. What is the most important limitation to communicate about this final model's expected performance?",The model will likely perform better on new data than the validation accuracy suggests,The validation accuracy is meaningless because kernels were varied,The validation accuracy may be optimistic because it was used to select the model,The model should be retrained from scratch before final evaluation,C,Repeated use of the validation set for model selection leads to optimistic performance estimates - the validation accuracy no longer represents true generalization performance.,"Option D is tempting because models are often retrained, but retraining does not address the optimism bias from repeated validation set use.", - 
CPMAI-P4-20250226-5739,Phase IV - Model Development,2,Document Model Development Activities,16,"A project team completes Phase IV and documents that three different algorithms were tested with 50 hyperparameter combinations total. The sponsor asks why this level of detail is necessary when only the final model matters. According to CPMAI, what should the project manager explain?",The detail is unnecessary and can be removed,Documenting the exploration path provides valuable learning for future projects and supports auditability,The documentation is required only for regulated industries,The detail proves the team did sufficient work to justify their budget,B,"Documenting the exploration path (algorithms tried, hyperparameters tested) creates organizational learning and supports auditability - future projects benefit from understanding what didn't work.","Option ? is tempting because documentation can justify effort, but the primary purpose is learning and reproducibility, not proof of work.", - 
CPMAI-P4-20250226-3907,Phase IV - Model Development,1,Build and Train Model,17,"In Phase IV, what is transfer learning as it relates to the Build and Train Model task?",A technique for transferring data from Phase II to Phase IV,A method for splitting training and test data across multiple phases,Transferring the trained model directly to production without validation,Using a pre-trained model as a starting point and fine-tuning it on the project dataset,D,"Transfer learning in Phase IV means using a pre-trained model (trained on a large general dataset) as a starting point and fine-tuning it on the project-specific dataset, leveraging existing knowledge to improve performance, especially with limited data.","Option A is tempting because 'transfer' could suggest moving data between phases, but transfer learning refers to leveraging pre-trained model knowledge, not data movement.", - 
CPMAI-P4-20250226-6452,Phase IV - Model Development,1,Select Modeling Technique,18,Which Phase IV activity involves documenting the rationale for choosing a particular algorithm over alternatives?,Build and Train Model,Select Modeling Technique,Perform Model Tuning and Validation,Split Datasets,B,"Select Modeling Technique includes not just choosing an algorithm but documenting the rationale, including why alternatives were rejected based on project requirements.",Option A (Build and Train) executes the chosen technique but doesn't include the selection rationale documentation., - 
CPMAI-P4-20250226-8135,Phase IV - Model Development,2,Perform Model Tuning and Validation,19,"A data scientist completes model tuning and achieves 88% accuracy on the validation set. She then evaluates the same model on the test set that was held out since Phase III and achieves 87% accuracy. According to CPMAI, what does this indicate?",The model generalizes well as test performance closely matches validation performance,The model is underfitting and needs more complex features,The model is overfitting and needs more tuning,The test set should be combined with training data to improve accuracy,A,Similar performance between validation (88%) and test (87%) indicates good generalization - the model performs consistently on unseen data that wasn't used for tuning.,"Option C is tempting because any drop might suggest overfitting, but a small drop (1%) between validation and test is normal and indicates good generalization.", - 
CPMAI-P4-20250226-2971,Phase IV - Model Development,3,Document Model Development Activities,20,A project team completes Phase IV and prepares documentation for the gate review. The data scientist wants to include code snippets showing how the model was trained. The project manager prefers a high-level summary without code. The governance reviewer notes that CPMAI requires reproducibility. What is the most appropriate approach?,Include only the high-level summary to keep documentation accessible,Include code snippets and sufficient detail to enable reproducing the model if needed,Documentation should include neither - the code itself is sufficient,Create separate documentation for technical and business audiences with different detail levels,B,CPMAI's reproducibility requirement means Phase IV documentation must include sufficient technical detail (including code or pseudocode) to enable the same model to be recreated if needed.,"Option D is tempting because different audiences have different needs, but the question is about meeting CPMAI's reproducibility requirement, which demands technical detail.","The distinction between summary documentation (for business stakeholders) and reproducibility documentation (for technical recreation) is important - CPMAI requires both, but reproducibility demands technical detail."
CPMAI-P4-20260226-2715,Phase IV - Model Development,2,Select Modeling Technique,1,"A retail analytics team has prepared customer data and is ready to build a churn model. Before they run any training jobs, the lead wants to choose the modeling technique (for example, decision trees vs neural networks) and record initial hyperparameter settings. In Phase IV, which task covers selecting the machine learning algorithm or cognitive-relevant modeling technique?",Usage of AutoML,Select cognitive-relevant algorithm / modeling technique,AI Pattern identification,Generate model test design,B,Task: Select cognitive-relevant algorithm / modeling technique is where Phase IV selects the modeling technique and documents initial parameter or hyperparameter settings.,"Option C is tempting because Phase I AI Pattern identification can guide the choice, but the actual algorithm or modeling technique selection is a Phase IV activity.", - 
CPMAI-P4-20260226-1187,Phase IV - Model Development,1,Select Modeling Technique,2,"In Phase IV, which task addresses whether combining multiple models would improve performance and documents the ensemble method and configuration chosen?",Hyperparameter Optimization,Model Training / Model Building,Ensemble Methods,Enhance & Augment data,C,Task: Ensemble Methods in Phase IV addresses deciding whether an ensemble is useful and how it should be configured.,"Option B is tempting because it produces models, but ensemble decisions and configuration belong to Ensemble Methods.", - 
CPMAI-P4-20260226-1034,Phase IV - Model Development,2,Model Test and Validation Design,3,"A hospital team is building a supervised classification model to flag sepsis risk. They have labeled data and are eager to start training, but they have not defined how they will separate training, test, and validation datasets to estimate model quality. In Phase IV, which task should they perform first to define the testing and validation procedure?",KPI Measurement,Model Training / Model Building,Hyperparameter Optimization,Generate model test design,D,"Task: Generate model test design defines the procedure to test model quality and validity, including how to split data into training, test, and validation sets.","Option B is tempting because training feels like progress, but CPMAI requires defining the test and validation approach before running training.", - 
CPMAI-P4-20260226-4973,Phase IV - Model Development,1,Select Modeling Technique,4,Which Phase IV task includes verifying that selected tools are capable of fine-tuning existing models when a pre-trained or foundation model will be used?,Ensemble Methods,Usage of Generative AI,Usage of AutoML,Fine-Tuning / Re-training of Pre-Trained Models,D,Task: Fine-Tuning / Re-training of Pre-Trained Models requires determining the fine-tuning approach and verifying tooling can fine-tune existing models.,"Option B is tempting because foundation models are often generative, but this question is about fine-tuning decisions and tool verification. Option A is tempting because ensemble decisions are also Phase IV, but Ensemble Methods addresses combining multiple models, not fine-tuning a single pre-trained model.", - 
CPMAI-P4-20260226-1731,Phase IV - Model Development,2,Hyperparameter Optimization,5,"An insurance team trains a fraud model and sees strong training performance but weak validation performance, suggesting overfitting. They decide to revise hyperparameter settings and run the build again, repeating until they believe they have the best model. In Phase IV, which task group directs this revise-and-iterate hyperparameter work?",Hyperparameter Optimization,Generate model test design,Model Training / Model Building,Clean data,A,Generic Task Group: Hyperparameter Optimization covers revising hyperparameter settings based on assessment and iterating model building until the best model(s) are found.,"Option C is tempting because it runs the training, but revising and documenting hyperparameter tuning belongs to Hyperparameter Optimization.", - 
CPMAI-P4-20260226-1678,Phase IV - Model Development,1,Model Test and Validation Design,6,Which Phase IV task artifact explicitly includes documenting k-fold cross validation settings (if that method is used)?,Model Performance results,Hyperparameter Optimization,Generate model test design,Verify Data Quality,C,Task: Generate model test design includes documenting k-fold cross validation settings as part of the test and validation design artifacts.,"Option B is tempting because cross validation is related to model performance, but k-fold settings are documented in the test design, not in hyperparameter tuning.", - 
CPMAI-P4-20260226-2389,Phase IV - Model Development,3,Select Modeling Technique,7,"A biotech team will use a pre-trained foundation model and plans to fine-tune it on an internal corpus to improve domain performance. A teammate argues they should skip fine-tuning and only adjust prompts because it is faster. In Phase IV, which task is responsible for defining and documenting the approach to fine-tune or re-train the pre-trained model?",Usage of Generative AI,Fine-Tuning / Re-training of Pre-Trained Models,Usage of AutoML,Model Training / Model Building,B,Task: Fine-Tuning / Re-training of Pre-Trained Models governs deciding how the pre-trained model will be fine-tuned or re-trained and documenting that approach.,"Option A is the trap because prompt engineering is part of generative AI usage, but fine-tuning or re-training changes the model weights rather than just the prompts.",Fine-tuning or re-training a pre-trained model is distinct from prompt engineering and chaining approaches used under Usage of Generative AI.
CPMAI-P4-20260226-3390,Phase IV - Model Development,2,Model Training / Model Building,8,"A logistics team has selected its modeling technique and completed its test and validation design. Now the team runs the modeling tool on the prepared dataset to produce candidate models for this iteration. In Phase IV, which task covers conducting the model training or model building to create one or more models and/or model ensembles?",Generate model test design,Select cognitive-relevant algorithm / modeling technique,Model Training / Model Building,Hyperparameter Optimization,C,Task: Model Training / Model Building covers conducting training or building to create one or more models and/or model ensembles using the prepared dataset.,"Option B is tempting because algorithm choice is closely related, but training and producing the models is performed in Model Training / Model Building.", - 
CPMAI-P4-20260226-3042,Phase IV - Model Development,1,Select Modeling Technique,9,"In Phase IV, which task requires detailing a prompt engineering approach and, if using a third-party API, documenting access method, incremental costs, and limitations?",Operationalization plan,Fine-Tuning / Re-training of Pre-Trained Models,Usage of AutoML,Usage of Generative AI,D,"Task: Usage of Generative AI requires documenting the generative AI approach, including prompt engineering and any third-party API access details, costs, and limitations.","Option B is tempting because some generative models are pre-trained, but the question is about prompt engineering and API documentation, which falls under Usage of Generative AI.", - 
CPMAI-P4-20260226-4096,Phase IV - Model Development,2,Model Test and Validation Design,10,"A fintech team wants to validate a supervised model with k-fold cross validation, but their chosen tool cannot support the needed testing and validation approach. In Phase IV, which task explicitly includes verifying that selected tools support the methods of testing and validation?",Generate model test design,Select cognitive-relevant algorithm / modeling technique,Hyperparameter Optimization,Describe Data,A,Task: Generate model test design includes verifying that selected tools support the intended methods of testing and validation.,"Option B is tempting because tool choice often comes up during technique selection, but the verification of testing and validation tool support belongs to Generate model test design.", - 
CPMAI-P4-20260226-3718,Phase IV - Model Development,1,Hyperparameter Optimization,11,"Within the Generic Task Group: Hyperparameter Optimization, what must be documented after tuning is complete?",Prompt engineering strategy and chaining approach for LLM interactions,Final optimized hyperparameter settings with chosen values and the rationale or method used,"Data labeling method, costs, and labeling quality verification approach",Business success criteria and acceptable KPI thresholds,B,Task: Hyperparameter Optimization requires listing the final optimized hyperparameters and their chosen values along with the rationale for those choices.,"Option D is tempting because it sounds like acceptance criteria, but KPI thresholds and business success criteria are not Phase IV hyperparameter outputs.", - 
CPMAI-P4-20260226-1452,Phase IV - Model Development,2,Select Modeling Technique,12,"A telecom team decides to use AutoML to speed up model building for a churn project. They need to document how AutoML will be used, what the tool needs to operate, and how the outputs will be used, including ensuring AutoML outputs fit the I/O Flow determined earlier. In Phase IV, which task covers this work?",Usage of AutoML,Select cognitive-relevant algorithm / modeling technique,Generate model test design,Operationalization plan,A,Task: Usage of AutoML requires documenting how AutoML will be used and verifying that AutoML-created models are usable within the I/O Flow determined earlier.,"Option B is tempting because AutoML produces models, but CPMAI separates deciding to use AutoML and documenting its usage from the general algorithm selection task.", - 
CPMAI-P4-20260226-5827,Phase IV - Model Development,1,Model Test and Validation Design,13,"According to Phase IV, what is the primary purpose of the task Generate model test design?","Define a procedure to test model quality and validity, including how to split data into training, test, and validation sets",Train the model on the prepared dataset and produce candidate model artifacts,Select the algorithm and initial hyperparameter settings for the modeling technique,Measure business KPI outcomes to decide whether the model meets Phase I objectives,A,"Task: Generate model test design exists to define how model quality and validity will be tested, commonly by separating datasets for training, test, and validation.","Option D is tempting because it sounds like evaluation, but KPI Measurement is part of Phase V evaluation rather than Phase IV test design.", - 
CPMAI-P4-20260226-5384,Phase IV - Model Development,2,Select Modeling Technique,14,"A media company has multiple candidate models and is considering using bagging or boosting to improve performance. They need to decide if an ensemble is useful or feasible and document the ensemble method and configuration. In Phase IV, which task should they use?",Enhance & Augment data,Hyperparameter Optimization,Model Training / Model Building,Ensemble Methods,D,Task: Ensemble Methods covers determining whether an ensemble is needed and documenting the ensemble method and configuration.,"Option B is tempting because tuning can improve performance, but ensemble selection and configuration is handled by Ensemble Methods.", - 
CPMAI-P4-20260226-2046,Phase IV - Model Development,1,Model Training / Model Building,15,"In Phase IV, which task explicitly notes that the artifact is the actual models produced by the modeling tool, not a report?",Model Training / Model Building,Generate model test design,Hyperparameter Optimization,KPI Measurement,A,"Task: Model Training / Model Building produces the actual model artifacts and calls out that these are models, not a report.","Option D is tempting because it is about results, but KPI Measurement is a Phase V evaluation activity, not a Phase IV model-building task.", - 
CPMAI-P4-20260226-4521,Phase IV - Model Development,3,Model Test and Validation Design,16,A team reports 98% accuracy but trained and tested on the same dataset. They claim that their hyperparameter tuning process already addressed overfitting risk because they monitored training loss curves. They have not defined a separate validation or test split strategy. What is the missing Phase IV step?,Model Performance results,Hyperparameter Optimization,Generate model test design,Model Training / Model Building,C,"Task: Generate model test design defines the plan for training, testing, and validating models, including dataset splitting and validation approach to avoid overfit or underfit.","Option B is the trap because the team claims tuning addressed overfitting, but without a test and validation design (including data splits), monitoring training loss alone cannot credibly validate generalization.",Designing testing and validation (including data splits and cross validation) is the boundary that must precede tuning; tuning does not substitute for a validation plan.
CPMAI-P4-20260226-8365,Phase IV - Model Development,2,Hyperparameter Optimization,17,"A manufacturing team wants to maximize model fit and decides to use a specific hyperparameter optimization method, then record how fit changes across training, test, and validation data after each revision. In Phase IV, which task group covers selecting the optimization method and documenting these tuning results?",Select cognitive-relevant algorithm / modeling technique,Model Training / Model Building,Generate model test design,Hyperparameter Optimization,D,"Generic Task Group: Hyperparameter Optimization includes determining methods for hyperparameter optimization, performing optimization, and documenting measurements of model fit across datasets.","Option B is tempting because training runs are involved, but choosing optimization methods and documenting tuning revisions belongs to Hyperparameter Optimization.", - 
CPMAI-P4-20260226-7012,Phase IV - Model Development,1,Model Training / Model Building,18,"Which Phase IV task requires you to describe the resulting models, interpret them, and document any difficulties encountered with their meanings?",Hyperparameter Optimization,Model Training / Model Building,Generate model test design,Usage of AutoML,B,Task: Model Training / Model Building includes producing model descriptions that interpret the resulting models and document difficulties with their meanings.,"Option A is tempting because tuning and interpretation often happen together, but the explicit requirement to describe and interpret resulting models belongs to Model Training / Model Building.", - 
CPMAI-P4-20260226-6189,Phase IV - Model Development,3,Hyperparameter Optimization,19,"A project lead says the model is ready because business KPI lift looks good, but the team has not documented any measurements of model fit against training, test, and validation data during tuning. In Phase IV, which specific Hyperparameter Optimization requirement is being missed?",KPI Measurement,"Document measurements of model fit against training, test, and validation data",Model Performance results,Determine business success criteria,B,"Hyperparameter Optimization explicitly requires documenting measurements of model fit against training, test, and validation data as part of tuning and iteration.","Option A is the trap because KPI Measurement sounds like proof, but KPI Measurement is a Phase V activity and does not replace the Phase IV requirement to document model fit against data splits.",Technical fit documentation during tuning is a Phase IV requirement that is distinct from Phase V business KPI evaluation.
CPMAI-P4-20260226-9147,Phase IV - Model Development,3,Model Training / Model Building,20,"A team has finished hyperparameter tuning and now needs deliverables that can be shared in later phases: the actual model artifacts plus written descriptions explaining what the models mean and what was difficult to interpret. A stakeholder insists that tuning documentation is enough. In Phase IV, which task is responsible for producing the models and their model descriptions?",Generate model test design,Hyperparameter Optimization,Model Training / Model Building,Usage of Generative AI,C,Task: Model Training / Model Building produces the actual models and requires describing and interpreting the resulting models for future phases or iterations.,"Option B is the trap because tuning is recent and important, but Hyperparameter Optimization documents parameter choices, while model artifacts and descriptions are produced by Model Training / Model Building.","Producing model artifacts and model descriptions is a Model Training / Model Building output, while Hyperparameter Optimization focuses on parameter settings and tuning documentation."
CPMAI-P5-20250226-7184,Phase V - Model Evaluation,1,Evaluate Model Performance Results,1,"Which Phase V task involves assessing the model's technical accuracy using metrics like precision, recall, and F1 score on the held-out test dataset?",Perform KPI Measurement,Conduct Review Process,Evaluate Model Performance Results,Document Evaluation Findings,C,Evaluate Model Performance Results is the Phase V task focused on technical assessment using metrics calculated on the test dataset that was held back since Phase III.,"Option A (KPI Measurement) is tempting because it's also evaluation, but it measures business outcomes, not technical performance metrics.", - 
CPMAI-P5-20250226-3529,Phase V - Model Evaluation,2,Perform KPI Measurement,2,"A fraud detection model achieves 98% accuracy on the test set. However, when deployed in a pilot, it only catches 40% of actual fraud cases, falling short of the 75% fraud capture rate target established in Phase I. According to CPMAI, what should happen next?",Proceed to Phase VI since technical accuracy is excellent,Return to Phase IV to retrain the model before the Review Process,Document the KPI shortfall and proceed if the sponsor approves,The model fails the KPI Measurement gate and cannot proceed to Phase VI,D,KPI Measurement is an independent evaluation gate in Phase V - the model must meet both technical performance thresholds AND business KPI targets to receive a Go decision.,"Option B is tempting because retraining might help, but the immediate action is failing the gate and determining root cause, not automatically returning to Phase IV.", - 
CPMAI-P5-20250226-9641,Phase V - Model Evaluation,3,Conduct Review Process,3,"A project team completes technical evaluation and KPI measurement. The model meets all technical thresholds but achieves only 92% of the business KPI target. The sponsor argues that 92% is close enough and wants to approve moving to Phase VI. According to CPMAI, what is the most appropriate response?",Accept the sponsor's decision as the business owner,Document the 8% gap and proceed with a conditional approval,The Review Process gate cannot be completed with a Go decision until all thresholds are met independently,Escalate to the PMO for a waiver decision,C,"The Review Process gate requires independent satisfaction of both technical and business evaluation criteria - a Go decision requires both gates to pass, regardless of how close the KPI came.","Option B is tempting because conditional approvals happen in practice, but CPMAI's governance model requires meeting all thresholds before the Review Process gate can issue a Go.","The independence of evaluation gates is the core Phase V concept - technical pass does not compensate for KPI shortfall, and vice versa."
CPMAI-P5-20250226-5278,Phase V - Model Evaluation,2,Document Evaluation Findings,4,"A project team completes Phase V evaluation and prepares documentation for the gate review. The documentation includes test set performance metrics, KPI measurement results, and comparisons to the baseline model. What is the primary purpose of this documentation?",To satisfy regulatory compliance only,To provide input to Phase VI Model Operationalization and enable deployment decisions,To replace the need for ongoing monitoring,To serve as the final project closure report,B,Document Evaluation Findings captures all evaluation results specifically to inform Phase VI deployment decisions and establish baseline performance for ongoing monitoring.,"Option A is tempting because compliance is important, but the primary purpose is handoff to operationalization and establishing performance baselines.", - 
CPMAI-P5-20250226-1935,Phase V - Model Evaluation,1,Evaluate Model Performance Results,5,"During Phase V evaluation of a binary classification model with imbalanced classes, which metric is generally more informative than accuracy when assessing model performance results?",Mean Absolute Error,R-squared,F1 Score,Root Mean Squared Error,C,F1 Score (harmonic mean of precision and recall) is more informative than accuracy for imbalanced classification problems because accuracy can be misleading when one class dominates. This is a key consideration when evaluating model performance results in Phase V.,"Options A, B, and D (MAE, R-squared, RMSE) are tempting but are regression metrics, not classification metrics - a Phase V evaluator must select the appropriate metric family for the problem type.", - 
CPMAI-P5-20250226-4291,Phase V - Model Evaluation,1,Perform KPI Measurement,6,"According to CPMAI, what is the primary input that defines what the Perform KPI Measurement task evaluates in Phase V?",Phase IV model performance logs,Business success criteria and KPIs defined in Phase I,Stakeholder feedback collected during the Phase V Review Process,Industry benchmark datasets for the relevant domain,B,Perform KPI Measurement evaluates the model against the business success criteria and KPIs originally defined in Phase I Business Understanding. These Phase I criteria serve as the benchmark for determining whether the model delivers the intended business impact.,"Option A is tempting because the Phase IV model is what is being measured, but the criteria defining what to measure come from Phase I, not Phase IV.", - 
CPMAI-P5-20250226-4820,Phase V - Model Evaluation,2,Conduct Review Process,7,"A project team presents a model at the Phase V gate review. Technical evaluation shows 85% accuracy (exceeds 80% threshold). KPI measurement shows 18% cost reduction (meets 15% target). The project sponsor wants to approve moving to Phase VI. According to CPMAI, what must occur before the Go decision is finalized?",Immediate approval since both thresholds are met,Documentation sign-off from all stakeholders,The Review Process gate must be formally completed with all findings documented and approved,Proceed to Phase VI and complete documentation later,C,"Even when thresholds are met, the Review Process gate requires formal completion with all evaluation findings documented, reviewed, and approved before a Go decision is issued.","Option A is tempting because thresholds are met, but CPMAI requires formal gate completion, not just threshold satisfaction.", - 
CPMAI-P5-20250226-6153,Phase V - Model Evaluation,1,Document Evaluation Findings,8,Which Phase V document typically contains the comparison between the new model's performance and the current baseline or champion model?,Data Quality Report,Model Development Summary,Model Evaluation Report,Project Charter,C,"The Model Evaluation Report documents all Phase V findings, including comparisons to baseline models, technical metrics, and KPI measurement results.",Option B (Model Development Summary) is a Phase IV document. Option A (Data Quality Report) is a Phase II document., - 
CPMAI-P5-20250226-2974,Phase V - Model Evaluation,2,Evaluate Model Performance Results,9,"A credit scoring model is evaluated on the test set and achieves 90% accuracy. However, analysis shows the model incorrectly denies loans to 30% of creditworthy applicants from certain demographic groups. Which additional metric should the team examine to understand this issue?",Accuracy,Precision and recall by subgroup,Overall F1 score,Root Mean Squared Error,B,"Evaluating performance metrics (precision, recall) across different subgroups is essential for identifying fairness and bias issues that overall accuracy masks.",Option C (Overall F1) would also mask subgroup disparities. Option A (Accuracy) is the metric already reported., - 
CPMAI-P5-20250226-7830,Phase V - Model Evaluation,1,Perform KPI Measurement,10,"In CPMAI Phase V, what is the key distinction between Evaluate Model Performance Results and Perform KPI Measurement?",They are two names for the same evaluation task,Model Performance evaluates technical metrics on the test set; KPI Measurement evaluates business impact against Phase I objectives,KPI Measurement is performed first and Model Performance evaluation follows,Model Performance evaluation is optional while KPI Measurement is mandatory,B,"Evaluate Model Performance Results focuses on technical metrics (precision, recall, accuracy, etc.) calculated on the held-out test dataset, while Perform KPI Measurement assesses whether the model achieves the business KPIs and success criteria defined in Phase I Business Understanding.","Option D is tempting because KPI Measurement is often emphasized as a critical gate, but both tasks are required components of Phase V evaluation under the CPMAI framework.", - 
CPMAI-P5-20250226-5137,Phase V - Model Evaluation,1,Conduct Review Process,11,What is the purpose of the Review Process gate at the end of Phase V?,To approve the project budget for Phase VI,To make a formal Go/No-Go decision based on evaluation results,To select the modeling technique for Phase IV,To document data quality issues,B,The Review Process gate is the formal decision point where stakeholders evaluate Phase V results and make a Go/No-Go decision about proceeding to Phase VI operationalization.,"Option A is tempting because budget is reviewed, but the primary purpose is the Go/No-Go decision based on evaluation results.", - 
CPMAI-P5-20250226-7489,Phase V - Model Evaluation,2,Document Evaluation Findings,12,A project team completes Phase V evaluation and documents that the new model achieves 88% accuracy compared to the current champion model's 82% accuracy. They also document that the model was evaluated on a test set from Q3 data while the champion was evaluated on full-year data. Why is this documentation important?,It proves the new model is superior,It provides context for fair comparison and identifies potential limitations in the evaluation,It satisfies a regulatory requirement for all projects,It replaces the need for a Phase VI pilot,B,Documenting evaluation context (like different time periods) is essential for interpreting results fairly and identifying limitations that might affect the Go/No-Go decision.,"Option A is tempting because the numbers show improvement, but without context (different time periods), the comparison may be misleading - documentation provides that context.", - 
CPMAI-P5-20250226-3605,Phase V - Model Evaluation,3,Evaluate Model Performance Results,13,"A churn prediction model is evaluated on the test set and achieves precision of 0.75 and recall of 0.60. The business requirements from Phase I state that false positives (offering retention discounts to non-churners) cost $10 each, while false negatives (missing actual churners) cost $100 each in lost revenue. According to CPMAI, what is the most accurate assessment?",The model fails because recall is below 0.80,"The model should be evaluated against business-weighted metrics that account for asymmetric costs, not just standard thresholds",Precision and recall are equally important regardless of business context,The model passes because both metrics are above 0.50,B,"Phase V evaluation must consider business context - when costs of different error types are asymmetric, evaluation should use weighted metrics or appropriate thresholds that reflect business impact.","Option A is tempting because recall thresholds are common, but the business context (cost asymmetry) should drive the evaluation criteria, not arbitrary thresholds.","The integration of business context into technical evaluation is a key Phase V concept - evaluation criteria must reflect Phase I business requirements, not just standard metrics."
CPMAI-P5-20250226-6517,Phase V - Model Evaluation,2,Perform KPI Measurement,14,"A customer segmentation project successfully builds a model that creates five distinct customer segments. The Phase I KPI was to enable targeted marketing campaigns. During KPI measurement, the marketing team confirms they can now create segment-specific campaigns using the model output. According to CPMAI, what should be documented as part of this KPI evaluation?",Only that the technical clustering metrics were strong,Only that the KPI was met with a pass/fail notation,"The specific business value enabled, including how the segments support targeted campaigns and the evidence of business impact",That the model should proceed directly to Phase VI without further review,C,"KPI Measurement documentation should capture not just whether the KPI was met, but how the model delivers business value - the specific campaigns enabled by the segments provide evidence of measurable business impact that informs the Review Process gate.","Option B is tempting because confirming the KPI was met seems sufficient, but CPMAI requires documenting the evidence and specifics of how business value is realized, not just a binary pass/fail.", - 
CPMAI-P5-20250226-1847,Phase V - Model Evaluation,1,Evaluate Model Performance Results,15,"When a Phase V team evaluates a regression model predicting housing prices, which metric measures error as the square root of the average squared differences between predicted and actual values?",Mean Absolute Error (MAE),R-squared (R²),Root Mean Squared Error (RMSE),Accuracy,C,"RMSE measures the square root of the average squared differences between predictions and actual values, providing error in the same units as the target variable. It is a standard Phase V metric for evaluating regression model performance results.","Option A (MAE) is tempting because it is also a regression error metric, but it measures average absolute difference rather than the square root of average squared difference.", - 
CPMAI-P5-20250226-6392,Phase V - Model Evaluation,3,Conduct Review Process,16,"A project team presents a model at the Phase V gate review. Technical evaluation passes, KPI measurement passes, and documentation is complete. However, the project sponsor is unavailable for the next two weeks. The project manager wants to proceed to Phase VI to avoid delays. According to CPMAI, what should happen?",Proceed to Phase VI and have the sponsor review documentation later,Delay the Phase VI start until the sponsor can formally participate in the Review Process,Have a delegate approve in the sponsor's absence,Proceed with Phase VI planning but delay actual deployment,B,"The Review Process gate requires appropriate stakeholder participation, including the project sponsor - proceeding without formal review violates the governance framework regardless of results.","Option A is tempting because results are positive and delays are costly, but CPMAI governance requires formal stakeholder review before exiting Phase V.",The governance principle that process matters as much as results is critical - positive evaluations don't bypass the need for formal review with appropriate participants.
CPMAI-P5-20250226-4753,Phase V - Model Evaluation,2,Document Evaluation Findings,17,A project team completes Phase V and documents that the new model outperforms the current champion on all metrics. They also document that the evaluation used a test set from 2024 data while the champion was evaluated on 2023 data. The sponsor asks why the champion wasn't re-evaluated on the same test set. What should the project manager explain?,Re-evaluating the champion would have been extra work,Comparing both models on the same test set ensures a fair A/B comparison and is the correct CPMAI approach,The champion's performance on 2023 data is sufficient for comparison,The new model should only be compared to itself over time,B,Proper champion/challenger evaluation requires testing both models on the same hold-out dataset to ensure fair comparison - this is the correct Phase V approach.,"Option C is tempting because historical performance is available, but comparing across different time periods introduces confounding variables that invalidate the comparison.", - 
CPMAI-P5-20250226-8015,Phase V - Model Evaluation,1,Perform KPI Measurement,18,Which Phase V activity involves measuring the model's impact against the business objectives established in Phase I?,Evaluate Model Performance Results,Perform KPI Measurement,Conduct Review Process,Document Evaluation Findings,B,Perform KPI Measurement is specifically focused on assessing the model's business impact against the KPIs and success criteria defined during Phase I Business Understanding.,"Option A (Model Performance) measures technical metrics, not business impact.", - 
CPMAI-P5-20250226-2936,Phase V - Model Evaluation,2,Evaluate Model Performance Results,19,"A fraud detection model is evaluated and shows precision of 0.95 (when it predicts fraud, it's correct 95% of the time) but recall of 0.40 (it only catches 40% of actual fraud). The fraud investigation team is overwhelmed with false positives from the current system. According to CPMAI, which aspect of this performance is most relevant to the business context?",Recall is too low and the model fails,Precision is excellent and meets the business need of reducing investigator workload,Both metrics are important but precision aligns with the stated business pain point,The model should be rejected until recall improves,C,"The business context (overwhelmed investigators) suggests reducing false positives (improving precision) is the priority, but both metrics matter - evaluation must consider which metric aligns with the primary business pain point.","Option B is tempting because precision directly addresses the stated pain, but CPMAI requires balanced evaluation - high precision with very low recall might miss too much fraud.", - 
CPMAI-P5-20250226-5608,Phase V - Model Evaluation,3,Document Evaluation Findings,20,"A project team completes Phase V with a No-Go decision because the model failed to meet KPI targets. The sponsor wants to discard all documentation since the project is not moving forward. According to CPMAI, what is the most appropriate response?","Archive the evaluation documentation but restrict access, since sharing failure details could discourage future AI investment",Archive the complete documentation as a learning asset and make it accessible for future project teams,Return the documentation to the Phase IV team to allow them to iterate on the model and resubmit for evaluation,"Archive only the technical evaluation and KPI results, discarding the Review Process notes since no Go decision was issued",B,Documentation of failed projects is valuable organizational learning - understanding why a model did not meet KPIs informs future projects and prevents repeating mistakes. CPMAI emphasizes that all Phase V outcomes contribute to organizational knowledge.,"Option A is tempting because protecting team morale is a real concern, but CPMAI's learning culture requires transparency - restricting access defeats the purpose of organizational learning from failures.","The principle that documentation serves organizational learning, not just project continuation, is a key Phase V concept - restricting or discarding failure documentation undermines organizational maturity regardless of the No-Go outcome."
CPMAI-P5-20260226-1057,Phase V - Model Evaluation,2,Evaluate Model Performance Results,1,"A subscription e-commerce team trains three candidate churn-prediction models using different algorithms. During Phase V, the team produces learning curves, ROC curves, and precision-recall charts for each model. They then rank the models against the evaluation criteria established earlier and select the top performer. Which Phase V task group does this activity belong to?",Perform KPI Measurement,Document Evaluation Findings,Conduct Review Process,Evaluate Model Performance Results,D,"Evaluate Model Performance Results is the Phase V task group focused on producing validation measures (ROC curves, learning curves, precision-recall) and ranking candidate models against evaluation criteria. This is the technical evaluation of model quality, not the business KPI assessment.","Option A (Perform KPI Measurement) is tempting because it also involves evaluation, but KPI Measurement specifically assesses the model against Phase I business success criteria and KPI targets, not technical validation metrics and model ranking.", - 
CPMAI-P5-20260226-1386,Phase V - Model Evaluation,1,Evaluate Model Performance Results,2,"During Phase V Evaluate Model Performance Results, which of the following is a primary output?",A deployment-ready production pipeline,A project charter update with revised scope,An iteration plan specifying which earlier phases to revisit,A model validation report ranking candidate models against evaluation criteria,D,"Evaluate Model Performance Results produces the model validation report containing evaluation measures (confusion matrices, ROC curves, precision-recall) and ranks resulting models against the defined evaluation criteria.","Option C is tempting because iteration planning is a Phase V concept, but it belongs to the broader evaluation findings and review activities, not to the core model performance evaluation output.", - 
CPMAI-P5-20260226-1764,Phase V - Model Evaluation,2,Conduct Review Process,3,"A healthcare team is ready to move a diagnostic model to operationalization. However, the compliance officer asks whether any disallowed patient attributes were used in training and whether all required attributes will be available in the production data pipeline. According to CPMAI, which Phase V task group addresses these quality assurance concerns?",Evaluate Model Performance Results,Perform KPI Measurement,Conduct Review Process,Document Evaluation Findings,C,"Conduct Review Process is the Phase V task group that includes quality assurance checks such as verifying that only allowed attributes were used, confirming future data availability, and identifying any overlooked factors before operationalization.","Option A is tempting because it involves model assessment, but Evaluate Model Performance Results focuses on technical validation metrics and model ranking, not compliance and QA checks about allowed attributes.", - 
CPMAI-P5-20260226-2149,Phase V - Model Evaluation,1,Document Evaluation Findings,4,"According to CPMAI, what is the primary purpose of Document Evaluation Findings in Phase V?","To capture all evaluation results, iteration requirements, and review outcomes so that deployment decisions and future iterations are properly informed",To perform hyperparameter tuning on the selected model,To define the business KPIs that the model must meet,To build the production monitoring dashboard,A,"Document Evaluation Findings consolidates all Phase V outputs — technical performance results, KPI measurement outcomes, review process findings, and iteration requirements — into documentation that informs Phase VI deployment decisions and supports organizational learning.","Option C is tempting because KPIs are central to Phase V, but defining business KPIs is a Phase I Business Understanding activity, not a Phase V documentation task.", - 
CPMAI-P5-20260226-2483,Phase V - Model Evaluation,3,Evaluate Model Performance Results,5,"A retail demand-forecasting team trains three candidate models. Model A achieves the best RMSE on the test set. Model B has slightly worse RMSE but significantly better performance on tail events (extreme demand spikes). The Phase I business requirements emphasized avoiding stock-outs during peak periods. During Phase V evaluation, the team must rank the models. Which assessment is most accurate under CPMAI?",Model A should be ranked highest because RMSE is the standard regression metric and it achieved the best score,"Model B should be ranked highest because it better captures extreme demand events, which aligns with the Phase I stock-out avoidance requirement",Both models should be discarded and the team should iterate with new algorithms,Model ranking should be deferred until Phase VI pilot testing can determine real-world performance,B,"Evaluate Model Performance Results requires ranking models against evaluation criteria that reflect the business context. When Phase I requirements emphasize avoiding stock-outs during peak periods, performance on tail events is more aligned with the business need than overall RMSE, making Model B the better-ranked candidate.","Option A is the primary trap because RMSE is a standard metric and Model A has the best score, which seems technically correct. However, CPMAI requires evaluation criteria to reflect Phase I business requirements, not just standard metric rankings.","The distinction between standard metric rankings (RMSE) and business-aligned evaluation criteria is the core discrimination — Evaluate Model Performance Results must rank models against criteria that trace back to Phase I business requirements, not default to standard metrics."
CPMAI-P5-20260226-3017,Phase V - Model Evaluation,2,Perform KPI Measurement,6,"An energy company deploys a predictive maintenance model in a three-month pilot. The model achieves 93% accuracy in predicting equipment failures on the test set. However, when measured against the Phase I KPI of reducing unplanned downtime by 25%, the pilot data shows only a 12% reduction. The technical lead argues the pilot was too short to demonstrate the full KPI impact. According to CPMAI, what is the correct Phase V assessment?","The model fails the Perform KPI Measurement gate because the Phase I business KPI target was not met, regardless of the technical accuracy explanation",The model passes Phase V because technical accuracy exceeds the threshold,The KPI should be revised downward to match the pilot results,The team should skip KPI Measurement and proceed based on the strong technical metrics,A,Perform KPI Measurement evaluates the model against the business success criteria and KPI targets defined in Phase I. Technical accuracy (93%) and business KPI achievement (12% vs 25% target) are independent evaluation gates — failing the business KPI means the model does not pass KPI Measurement regardless of strong technical performance.,"Option B (the model passes because technical accuracy exceeds the threshold) is the primary trap because 93% accuracy is strong and teams often conflate technical performance with business impact. However, CPMAI requires independent satisfaction of both technical metrics and business KPIs in Phase V.", - 
CPMAI-P5-20260226-3274,Phase V - Model Evaluation,1,Conduct Review Process,7,"According to CPMAI, which Phase V task group explicitly requires verifying that only allowed attributes were used in the model and that those attributes will be available for future production analyses?",Conduct Review Process,Evaluate Model Performance Results,Perform KPI Measurement,Document Evaluation Findings,A,"Conduct Review Process includes quality assurance checks such as confirming that only allowed attributes were used, verifying future attribute availability, and identifying any overlooked factors that could affect operationalization.","Option B is tempting because attribute usage affects model performance, but Evaluate Model Performance Results focuses on technical validation metrics and model ranking, not on compliance and QA verification of attribute usage rules.", - 
CPMAI-P5-20260226-3610,Phase V - Model Evaluation,2,Document Evaluation Findings,8,"A logistics optimization team completes Phase V evaluation. The model meets technical thresholds but only marginally achieves the KPI target. Stakeholders want a structured record of what should change in data collection, feature engineering, or modeling approach if the team iterates to improve results. According to CPMAI, where should this iteration plan be captured?",In the Phase IV Model Development log,"In the Document Evaluation Findings output, which includes required changes and the iteration approach",In the Phase VI Monitoring and Maintenance plan,In the Perform KPI Measurement summary,B,"Document Evaluation Findings captures all Phase V outcomes including required model, data, or process changes and the approach for iteration back to Phases II, III, or IV. This documentation ensures that if iteration is needed, the rationale and plan are formally recorded as part of Phase V outputs.","Option D is tempting because the KPI shortfall drives the iteration need, but Perform KPI Measurement evaluates against targets — it does not document the iteration plan, which belongs in the broader evaluation findings documentation.", - 
CPMAI-P5-20260226-4028,Phase V - Model Evaluation,1,Perform KPI Measurement,9,"In CPMAI Phase V, Perform KPI Measurement evaluates the model against which set of criteria?",The business success criteria and KPI targets defined in Phase I,The technical validation metrics calculated on the test set,The data quality thresholds established in Phase II,The hyperparameter configurations selected in Phase IV,A,"Perform KPI Measurement specifically evaluates the model against the business success criteria, business KPIs, and technology KPIs defined during Phase I Business Understanding. It assesses whether the model delivers the intended business impact, not just technical accuracy.","Option B is the primary trap because technical metrics are also evaluated in Phase V, but they belong to Evaluate Model Performance Results, not Perform KPI Measurement.", - 
CPMAI-P5-20260226-4475,Phase V - Model Evaluation,2,Evaluate Model Performance Results,10,"A telecom company evaluates a customer churn model that achieves strong overall accuracy of 91%. However, closer analysis reveals the model has much higher false negative rates for enterprise customers than for consumer customers. During Phase V Evaluate Model Performance Results, the team reviews precision, recall, and F1 by customer segment. What does this subgroup analysis primarily help the team assess?",Whether the model meets Phase I KPI targets for revenue retention,Whether the model needs hyperparameter tuning to improve enterprise segment performance,"Whether the model's technical performance is consistent across meaningful subgroups, revealing potential bias or fairness issues",Whether the Conduct Review Process gate can be completed,C,Evaluating performance metrics across subgroups is a key component of Evaluate Model Performance Results. Subgroup analysis reveals whether technical performance is consistent or whether the model exhibits bias — a critical finding that affects the model validation report and overall evaluation.,"Option A is tempting because enterprise customer churn directly affects revenue, but this analysis is about understanding technical performance consistency across groups (an Evaluate Model Performance Results concern), not measuring against a specific Phase I KPI target.", - 
CPMAI-P5-20260226-5091,Phase V - Model Evaluation,3,Perform KPI Measurement,11,"A fraud detection model achieves a precision of 0.97 and recall of 0.35 on the test set. The Phase I business KPI was defined as reducing fraud losses by 50%. In the pilot, fraud losses decreased by only 22% because the model misses most fraud cases despite being highly accurate when it does flag one. The technical lead argues precision is excellent and the model passes. According to CPMAI, what is the most accurate assessment?",The model passes because precision of 0.97 demonstrates excellent fraud identification capability,"The model fails Perform KPI Measurement because the 22% fraud loss reduction does not meet the 50% Phase I KPI target, regardless of how strong precision is",The team should revise the Phase I KPI to reflect achievable targets based on current model performance,The model passes Evaluate Model Performance Results but the Phase I KPI was poorly defined,B,Perform KPI Measurement is an independent gate that evaluates the model against the specific business KPIs defined in Phase I. A 22% reduction versus a 50% target is a clear KPI failure. Technical performance (high precision) is evaluated separately under Evaluate Model Performance Results and cannot compensate for a KPI shortfall.,"Option A is the primary trap because 0.97 precision sounds excellent, and teams often conflate strong technical metrics with business success. However, CPMAI maintains strict independence between technical evaluation and business KPI evaluation — passing one does not excuse failing the other.",The independence of Phase V evaluation gates is the core discrimination — Evaluate Model Performance Results (technical) and Perform KPI Measurement (business) are separate gates that must each be satisfied independently. Strong technical metrics cannot compensate for unmet business KPIs.
CPMAI-P5-20260226-5528,Phase V - Model Evaluation,1,Evaluate Model Performance Results,12,Which of the following evaluation outputs is specifically produced during Phase V Evaluate Model Performance Results?,A production monitoring dashboard,A list of required approvals for operationalization,Business KPI achievement summaries,"Confusion matrices, ROC curves, and precision-recall charts",D,"Evaluate Model Performance Results produces technical evaluation measures including confusion matrices, ROC curves, precision-recall charts, lift and gain charts, and learning curves. These are the core diagnostic outputs for assessing model quality.","Option C is tempting because KPI summaries are a Phase V output, but they are produced by Perform KPI Measurement, not by Evaluate Model Performance Results, which focuses on technical validation diagnostics.", - 
CPMAI-P5-20260226-6034,Phase V - Model Evaluation,2,Conduct Review Process,13,"A retail analytics sponsor wants to skip the Phase V review and proceed directly to operationalization because the dashboard shows all metrics in green. The project manager insists that CPMAI requires a thorough review to confirm nothing was overlooked, identify any activities that should be repeated, and detail the approvals required before production deployment. Which Phase V task group supports the project manager's position?",Evaluate Model Performance Results,Perform KPI Measurement,Conduct Review Process,Document Evaluation Findings,C,"Conduct Review Process is the Phase V task group that performs a thorough completeness and quality assurance review, identifies overlooked factors or activities that should be repeated, and details the required approvals before the model can be operationalized.","Option D is tempting because documentation captures review outcomes, but Document Evaluation Findings records what was found — it is Conduct Review Process that performs the actual QA review and identifies gaps and required approvals.", - 
CPMAI-P5-20260226-6712,Phase V - Model Evaluation,1,Document Evaluation Findings,14,"In CPMAI Phase V, which task group is responsible for documenting required model, data, or process changes and specifying which earlier phases must be revisited if iteration is needed?",Evaluate Model Performance Results,Conduct Review Process,Perform KPI Measurement,Document Evaluation Findings,D,"Document Evaluation Findings captures all Phase V conclusions including required changes to model, data, or process and the iteration approach specifying which earlier phases (II, III, or IV) need to be revisited to improve results.","Option B is tempting because the Review Process may trigger the recognition that changes are needed, but the actual documentation of what changes are required and which phases to revisit belongs to Document Evaluation Findings.", - 
CPMAI-P5-20260226-7249,Phase V - Model Evaluation,3,Document Evaluation Findings,15,"During a Phase V pilot, a predictive model's performance degrades when customer behavior shifts seasonally. The team wants to immediately build production monitoring dashboards to track drift. Leadership asks for a Phase V output that documents what must change and which earlier phases to revisit. Which statement is most accurate under CPMAI?",The team should proceed to Phase VI to build the monitoring dashboards since drift detection is an operationalization concern,"Document Evaluation Findings should capture the required changes and iteration approach, specifying which earlier phases need revisiting; production monitoring planning belongs to Phase VI",Conduct Review Process should build the monitoring dashboard as part of the Phase V quality assurance review,The team should skip documentation and iterate directly back to Phase III Data Preparation,B,"Document Evaluation Findings captures iteration requirements including what must change and which earlier phases to revisit. Production monitoring and drift detection dashboards are Phase VI Model Operationalization activities, not Phase V deliverables. Phase V documents the need; Phase VI implements the monitoring solution.","Option A is the primary trap because monitoring dashboards for drift detection do belong in Phase VI, but the question asks about the Phase V output — the team must first document the findings and iteration plan in Phase V before any Phase VI monitoring work begins.","The boundary between Phase V iteration documentation (Document Evaluation Findings) and Phase VI operational monitoring (monitoring and maintenance planning) is the core discrimination — Phase V documents what must change, Phase VI implements the ongoing monitoring solution."
CPMAI-P5-20260226-7683,Phase V - Model Evaluation,2,Perform KPI Measurement,16,"A telecom model improves the customer churn business KPI by 18%, exceeding the 15% Phase I target. However, it fails a technology KPI because model inference latency exceeds the 200ms constraint required by the real-time serving environment, a requirement also established in Phase I. According to CPMAI, what is the Perform KPI Measurement outcome?",The model passes because the business KPI was exceeded,The model passes because the business KPI is more important than the technology KPI,The model fails Perform KPI Measurement because both business and technology KPIs must be met,The technology KPI should be deferred to Phase VI as an operational concern,C,"Perform KPI Measurement evaluates the model against all KPI targets defined in Phase I, including both business KPIs and technology KPIs. Failing any Phase I KPI means the model does not pass the KPI Measurement gate, regardless of how well it performs on other KPIs.","Option A is the primary trap because the business KPI was exceeded (18% vs 15% target), and teams often prioritize business outcomes over technical constraints. However, CPMAI requires all Phase I KPIs — business and technology — to be independently satisfied.", - 
CPMAI-P5-20260226-8126,Phase V - Model Evaluation,1,Conduct Review Process,17,"According to CPMAI, what is the primary purpose of the Conduct Review Process task group in Phase V?","To perform quality assurance, identify overlooked factors, and detail required approvals before operationalization",To rank candidate models against evaluation criteria,To measure the model against Phase I business KPIs,To produce technical validation metrics like ROC curves and precision-recall charts,A,"Conduct Review Process performs a thorough quality assurance review, checks for overlooked factors (such as disallowed attributes or missing data availability), identifies activities that should be repeated, and details the approvals required before the model can move to Phase VI operationalization.","Option B (ranking candidate models against evaluation criteria) is tempting because model ranking is a Phase V activity, but it belongs to Evaluate Model Performance Results, not to the QA-focused Conduct Review Process.", - 
CPMAI-P5-20260226-8570,Phase V - Model Evaluation,2,Document Evaluation Findings,18,"A fraud analytics team completes Phase V evaluation with mixed results: the model meets technical thresholds but falls short on the fraud-loss-reduction KPI. The team determines that improving the training data with additional fraud patterns from a new data source would likely close the gap. According to CPMAI, where should the team document this finding and the plan to iterate back to Phase II Data Understanding and Phase III Data Preparation?",In the Phase II Data Quality Report as a retroactive amendment,In the Phase IV Model Development log alongside hyperparameter records,"In the Phase V Document Evaluation Findings output, which includes iteration requirements and the approach for revisiting earlier phases",In the Phase VI Monitoring and Maintenance plan as a post-deployment improvement,C,"Document Evaluation Findings is the Phase V task group responsible for capturing required model, data, or process changes and specifying the iteration approach, including which earlier phases (in this case Phase II and III) must be revisited to improve results.","Option D is tempting because improving the model could be seen as a future enhancement, but CPMAI treats iteration planning as a Phase V documentation requirement that must be captured before any Phase VI operationalization or post-deployment activity.", - 
CPMAI-P5-20260226-9005,Phase V - Model Evaluation,1,Perform KPI Measurement,19,Which statement accurately describes the scope of Perform KPI Measurement in CPMAI Phase V?,It evaluates only business KPIs and defers technology KPIs to Phase VI,It determines the production monitoring strategy for deployed models,"It focuses on technical validation metrics such as precision, recall, and RMSE",It evaluates the model against both business success criteria and technology KPI targets established in Phase I,D,"Perform KPI Measurement evaluates the model against the full set of Phase I criteria including business success criteria, business KPIs, and technology KPIs. It is not limited to business metrics — technology constraints defined in Phase I are also assessed during this task.","Option C is tempting because precision, recall, and RMSE are Phase V metrics, but they belong to Evaluate Model Performance Results (technical validation), not to Perform KPI Measurement (business and technology KPI assessment).", - 
CPMAI-P5-20260226-9462,Phase V - Model Evaluation,3,Conduct Review Process,20,"A model passes all technical validation measures and meets all KPI targets. During Conduct Review Process, the QA review discovers the model uses a customer credit score attribute that is on the organization's restricted-attribute list and will not be available in the production data pipeline. Two team members disagree on the next step. One says the team should document the issue and proceed to Phase VI since all metrics pass. The other says the Phase V gate cannot be cleared. Which assessment is most accurate under CPMAI?",Document the attribute issue in the evaluation findings and proceed to Phase VI since all quantitative thresholds were met,The Conduct Review Process gate cannot be cleared because the QA review identified a disallowed attribute — this must be resolved before operationalization regardless of metric results,Defer the attribute issue to Phase VI monitoring and address it post-deployment,Escalate only to the project sponsor for a waiver since the model performs well,B,"Conduct Review Process explicitly checks whether only allowed attributes were used and whether required attributes will be available in production. Discovering a restricted attribute is a QA finding that blocks the review gate — CPMAI governance requires resolving such issues before operationalization, regardless of how strong the quantitative metrics are.","Option A is the primary trap because the model objectively passes all quantitative gates, and documenting the issue while proceeding seems pragmatic. However, CPMAI's Conduct Review Process is specifically designed to catch exactly this type of compliance and data-availability issue — it is a blocking gate, not an advisory note.",The governance principle that Conduct Review Process is a blocking gate for compliance and QA issues is the core discrimination — quantitative success (passing all metrics and KPIs) does not override qualitative QA findings such as disallowed attributes or data availability problems identified during the review.
CPMAI-P6-20250226-3847,Phase VI - Model Operationalization,1,Plan Deployment,1,"Which Phase VI task involves defining the deployment strategy, integration points, rollback procedures, and stakeholder communication before the model goes live?",Deploy Model,Monitor Model Performance,Plan Deployment,Document Operationalization Activities,C,"Plan Deployment is the Phase VI task responsible for creating the comprehensive deployment plan covering strategy, integration, rollback, and communication.","Option A (Deploy Model) is tempting because deployment is the obvious activity, but planning must occur before execution.", - 
CPMAI-P6-20250226-4721,Phase VI - Model Operationalization,2,Deploy Model,2,"A credit scoring model has passed Phase V evaluation and received a Go decision. The project team integrates the model into the loan application system, configures it to score new applications in real-time, and verifies that predictions match Phase V validation results within acceptable tolerance. Which Phase VI task group does this activity belong to?",Plan Deployment,Monitor Model Performance,Document Operationalization Activities,Deploy Model,D,Deploy Model encompasses all activities to integrate the model into production systems and verify it is functioning correctly in the target environment.,"Option B (Monitor Model Performance) is tempting because monitoring follows deployment, but the scenario describes initial integration and verification, not ongoing performance tracking.", - 
CPMAI-P6-20250226-8392,Phase VI - Model Operationalization,1,Monitor Model Performance,3,"In Phase VI monitoring, what type of drift occurs when the underlying relationship between model inputs and outputs changes due to shifts in the real-world environment, even if the input data distributions remain the same?",Data drift,Concept drift,Model version drift,Feature drift,B,"Concept drift occurs when the relationship between inputs and outputs changes — for example, customer purchasing patterns change due to a new competitor entering the market, so the same inputs no longer predict the same outcomes. This is distinct from data drift, where input distributions shift.","Option A (Data drift) is tempting because both involve changes in the environment, but data drift refers specifically to changes in input data distributions, while concept drift refers to changes in the underlying input-output relationship.", - 
CPMAI-P6-20250226-1537,Phase VI - Model Operationalization,2,Document Operationalization Activities,4,"A project team completes the initial deployment of a churn prediction model. They create documentation containing deployment date, model version, integration details, and initial performance metrics. According to CPMAI, what is the primary purpose of this Phase VI documentation?",To establish a baseline for ongoing monitoring and enable future troubleshooting,To replace the need for a separate monitoring system,To satisfy regulatory requirements only,To serve as the final project closure report,A,Document Operationalization Activities captures deployment details specifically to establish baselines for monitoring and provide reference information for troubleshooting throughout the model's lifecycle.,"Option C is tempting because compliance is important, but the primary purpose is enabling effective monitoring and maintenance throughout the model's lifecycle.", - 
CPMAI-P6-20250226-6294,Phase VI - Model Operationalization,1,Plan Deployment,5,"During Phase VI Plan Deployment, which deployment strategy involves rolling out a new model to a small subset of users or transactions first before expanding to full production?",Big bang deployment,Direct cutover,Parallel run,Phased rollout,D,Phased rollout (or canary deployment) gradually increases exposure to limit risk and allow monitoring before full deployment. This strategy is defined during Plan Deployment as part of the overall deployment approach.,"Option C (Parallel run) is tempting because it also limits risk, but parallel run involves running old and new simultaneously for comparison, not rolling out to subsets incrementally.", - 
CPMAI-P6-20250226-7483,Phase VI - Model Operationalization,3,Deploy Model,6,"A project team is deploying a recommendation model for an e-commerce website. The data scientist proposes an A/B test where 10% of users see the new model's recommendations and 90% see the current model's recommendations during the first two weeks of deployment. The project manager objects, arguing that evaluation was already completed in Phase V and live testing is unnecessary. According to CPMAI, who is correct?",The project manager is correct — A/B testing belongs in Phase V,The data scientist is correct — A/B testing during deployment is a valid Phase VI strategy to validate model performance in the live production environment,"A/B testing is only appropriate for marketing campaigns, not model deployment",The test should be 50/50 to get statistically significant results,B,"A/B testing during initial deployment (Phase VI) is a valid strategy to validate model performance in the live environment before full rollout, complementing Phase V offline evaluation with real-world feedback.","Option A is tempting because evaluation occurs in Phase V, but live environment A/B testing is a deployment strategy that occurs in Phase VI, not a replacement for Phase V evaluation but a complement to it.",The distinction between offline evaluation (Phase V) and live validation (Phase VI) is critical — both serve different purposes and occur in different phases.
CPMAI-P6-20250226-3916,Phase VI - Model Operationalization,2,Monitor Model Performance,7,"A project team deploys a predictive maintenance model. Three months later, the monitoring dashboard shows that the average confidence score of predictions has dropped from 0.92 to 0.78, but accuracy against actual failures remains at 94%. According to CPMAI Phase VI monitoring practices, what should the team do?",Wait until failures occur to confirm if there is a real problem,Investigate the confidence score drop as a potential early warning sign of model degradation,Retrain the model immediately with new data,Ignore confidence scores since they do not measure actual accuracy,B,Monitoring includes proxy metrics (like confidence scores) as early warning indicators — a significant drop warrants investigation even before ground truth is available.,"Option A is tempting because waiting for ground truth seems prudent, but CPMAI monitoring includes proactive investigation of all available signals, not reactive waiting for failures.", - 
CPMAI-P6-20250226-5270,Phase VI - Model Operationalization,1,Document Operationalization Activities,8,"Which Phase VI document typically contains model version information, deployment dates, integration details, and monitoring thresholds?",Model Development Summary,Model Evaluation Report,Model Operations Log,Data Dictionary,C,"The Model Operations Log (or equivalent) tracks all operational details including versions, deployments, and monitoring configurations throughout the model's lifecycle.",Option A (Model Development Summary) is a Phase IV document. Option B (Model Evaluation Report) is a Phase V document., - 
CPMAI-P6-20250226-2643,Phase VI - Model Operationalization,2,Plan Deployment,9,"A healthcare diagnostics model has passed Phase V evaluation. The compliance officer requires proof that the model's decisions can be explained to regulators before it can be deployed. According to CPMAI, in which Phase VI task group should this requirement be addressed?",Plan Deployment — all requirements and constraints must be identified before deployment,Monitor Model Performance — it will be tracked after deployment,Deploy Model — it is a technical integration issue,Document Operationalization Activities — it will be recorded after deployment,A,"Plan Deployment includes identifying all requirements, constraints, and success criteria — including compliance requirements like explainability — before deployment execution begins.","Option C is tempting because explainability seems like a technical issue, but requirements identification occurs in planning, not during execution.", - 
CPMAI-P6-20250226-9851,Phase VI - Model Operationalization,3,Monitor Model Performance,10,"A credit risk model has been in production for one year. The monitoring system detects that the distribution of applicant income has shifted significantly — more high-income applicants are applying than when the model was trained. However, model accuracy, precision, and recall all remain within acceptable thresholds. According to CPMAI Phase VI monitoring, what is the most appropriate response?",No action is needed since performance metrics are stable,"This is data drift, which should be documented and monitored even without immediate performance impact, as it may indicate future risk",The model must be retrained immediately due to data drift,The monitoring system is likely reporting false alarms,B,Data drift (input distribution changes) should be documented and monitored even when performance has not yet degraded — it may indicate future risk or changing business conditions that will eventually affect model performance.,"Option A is tempting because stable performance suggests no immediate problem, but CPMAI emphasizes proactive monitoring of all drift types as potential early warning signals, regardless of current performance stability.",The principle that monitoring includes drift detection independent of performance is key — data drift matters even without immediate performance degradation because it may foreshadow future issues.
CPMAI-P6-20250226-4372,Phase VI - Model Operationalization,1,Deploy Model,11,What is the purpose of a rollback plan in Phase VI model deployment?,To revert to the previous model version if the new deployment causes issues,To improve model accuracy,To document deployment activities,To monitor model performance,A,A rollback plan defines the process for quickly reverting to the previous model version if the new deployment causes unexpected problems or performance degradation.,"Option D (monitor) is tempting because monitoring triggers rollbacks, but the rollback plan itself is a deployment contingency, not a monitoring activity.", - 
CPMAI-P6-20250226-7401,Phase VI - Model Operationalization,2,Document Operationalization Activities,12,"A project team deploys a model and updates the operations log with version 2.1. Three months later, they deploy version 2.2 with improved features. The team documents the change, including the reason for the update, performance comparison, and any configuration changes. Why is this version-to-version documentation important in CPMAI Phase VI?",It satisfies a one-time compliance requirement,It replaces the need for ongoing monitoring,It is only needed for regulated industries,It creates an audit trail of model evolution and supports troubleshooting,D,"Documenting model version changes creates an audit trail of the model's evolution, supports troubleshooting by linking issues to specific versions, and enables informed decisions about future model updates.","Option A is tempting because compliance is important, but the primary purpose is operational management and auditability throughout the lifecycle.", - 
CPMAI-P6-20250226-6138,Phase VI - Model Operationalization,3,Plan Deployment,13,"A project team has received a Go decision for a fraud detection model. The production environment requires 99.99% uptime, and any model errors could block legitimate transactions. The team proposes a shadow mode deployment where the model runs in parallel with the existing system, receiving live data and generating predictions, but without those predictions affecting actual transaction decisions. Which assessment is most accurate?",Shadow mode deployment is a Phase VI planning strategy that validates model performance in production conditions without risking live transactions,A phased rollout starting at 5% of transactions would be more appropriate than shadow mode because it generates real business impact data needed for validation,Shadow mode testing should have been completed during Phase V evaluation rather than Phase VI deployment,The team should deploy directly to production with an automated rollback trigger based on error rate thresholds,A,Shadow mode deployment (running in parallel without affecting decisions) is a valid Phase VI planning strategy for high-risk applications to validate performance in real production conditions without impacting live operations.,"Option B is the primary trap because phased rollout is also a legitimate deployment strategy and does generate real impact data. However, for 99.99% uptime requirements where errors block legitimate transactions, shadow mode is the more appropriate strategy because it eliminates transaction risk entirely during validation.","The distinction between deployment strategies based on risk tolerance is a key Phase VI concept — shadow mode eliminates production risk entirely during validation, while phased rollout accepts limited risk in exchange for real-world impact data. The scenario's uptime and transaction-blocking requirements favor shadow mode."
CPMAI-P6-20250226-8560,Phase VI - Model Operationalization,2,Monitor Model Performance,14,"A deployed recommendation model's accuracy against user click-through has declined from 85% to 72% over six months. The data scientist determines that consumer preferences have evolved due to seasonal trends and new product launches. The monitoring alert threshold was set at 80% accuracy. According to CPMAI Phase VI monitoring, what should happen next?",Trigger the model retraining or remediation process based on the degradation alert,Continue monitoring — fluctuations are normal,Immediately roll back to the previous model version,Disable the model until the cause is determined,A,"When monitoring detects significant performance degradation below thresholds, the appropriate response is to trigger the retraining process (or other remediation) as defined in the monitoring plan.","Option B is tempting because some fluctuation is normal, but a 13-percentage-point drop below the 80% threshold requires action, not continued passive monitoring.", - 
CPMAI-P6-20250226-1024,Phase VI - Model Operationalization,1,Deploy Model,15,"During Phase VI Deploy Model, which deployment approach runs the new model alongside the existing model and compares their outputs before switching production traffic completely?",Big bang deployment,Phased rollout,Parallel run,Canary deployment,C,"Parallel run operates both models simultaneously, comparing outputs to validate the new model before switching production traffic. This approach is executed during the Deploy Model task group.","Option B (Phased rollout) and D (Canary deployment) are similar but involve gradually shifting traffic to the new model, not running both models permanently in parallel for comparison.", - 
CPMAI-P6-20250226-5739,Phase VI - Model Operationalization,2,Document Operationalization Activities,16,"A project team retires a two-year-old customer segmentation model after deploying an improved replacement. The data science lead wants to simply delete the old model's artifacts and move on. However, the project manager argues that CPMAI requires documenting the retirement, including retirement date, reason, performance at retirement, and any lessons about model lifespan and degradation. Why is documenting model retirements important in Phase VI?",It is optional and only useful for compliance purposes,It is only needed for regulated industries,It proves the model was used,It maintains a complete model lineage and provides context for future projects,D,"Documenting model retirements completes the model lineage, provides historical context, and captures lessons learned about model lifespan and degradation patterns that inform future projects.","Option A is tempting because documentation often feels optional, but CPMAI emphasizes complete lifecycle documentation including retirement as part of Document Operationalization Activities.", - 
CPMAI-P6-20250226-3907,Phase VI - Model Operationalization,1,Monitor Model Performance,17,"In Phase VI monitoring, what condition exists when a model's technical performance metrics (accuracy, precision, recall) remain stable but the business outcomes the model was designed to improve have deteriorated?",Data drift,Feature drift,Model staleness,"A gap between technical performance and business impact, potentially caused by concept drift",D,A model can maintain stable technical metrics while business outcomes deteriorate — this occurs when the business context or definition of success changes (concept drift) while the historical labels used for accuracy measurement remain valid. Phase VI monitoring must track both technical metrics and business KPIs to detect this condition.,"Option A (Data drift) is tempting because environmental changes are involved, but data drift specifically refers to input distribution shifts, not to a divergence between technical metrics and business outcomes.", - 
CPMAI-P6-20250226-6452,Phase VI - Model Operationalization,1,Plan Deployment,18,Which Phase VI planning activity involves defining alert thresholds for model performance metrics and establishing response procedures for different types of alerts?,Model integration planning,Rollback procedure definition,Monitoring plan development,Stakeholder communication planning,C,"Monitoring plan development includes defining performance thresholds, alert conditions, and the procedures for responding to different types of alerts or degradation events.","Option B (Rollback procedure definition) is part of planning but addresses deployment failure contingencies, not the ongoing monitoring framework.", - 
CPMAI-P6-20250226-8135,Phase VI - Model Operationalization,2,Deploy Model,19,"A project team is deploying a model to a new geographic region. They successfully deploy the model, verify it is generating predictions, and confirm integration with local systems. According to CPMAI, what is the next Phase VI activity?",Close the project,Begin monitoring model performance,Plan the next deployment,Update the Phase I business case,B,"After successful deployment, the next Phase VI activity is to begin monitoring model performance to track ongoing behavior and detect any degradation.","Option A (Close the project) is tempting because deployment seems like the end, but monitoring is an ongoing Phase VI responsibility throughout the model operational life.", - 
CPMAI-P6-20250226-2971,Phase VI - Model Operationalization,3,Document Operationalization Activities,20,"A project team has been operating a model for two years. During an audit, the compliance team asks for proof that the model was monitored according to CPMAI requirements. The project manager provides the operations log showing monitoring dates, metrics checked, and results. The auditor says the documentation is incomplete. What is most likely missing?",The documentation is sufficient — logs show monitoring occurred,Documentation should focus only on anomalies and exceptions rather than routine monitoring checks,Accountability information — who performed each monitoring check — is required to meet governance standards,Only automated monitoring outputs need to be documented; manual review records are optional,C,"CPMAI documentation requirements include accountability — records should identify who performed monitoring activities, not just that they occurred, to support governance and enable follow-up when issues arise.","Option A is the primary trap because the log does contain substantial monitoring evidence (dates, metrics, results), and many teams would consider this complete. However, governance requires attribution to specific individuals to ensure accountability and enable follow-up questions during audits.","The principle that documentation must support accountability, not just record-keeping, is a key Phase VI governance concept — who performed each check matters as much as what was checked and when."
CPMAI-P6-20260226-1428,Phase VI - Model Operationalization,2,Model operationalization plan,1,"An airline has a validated demand-forecast model and wants it running in production across two regions. The team needs to decide whether the model runs in batch mode or as a real-time microservice and document the steps to deploy it with the required IT processes. In Phase VI, which task should produce this deliverable?",Operationalization plan,Monitoring and maintenance plan,KPI Measurement,Determine next steps,A,"Operationalization plan is the Phase VI task that takes evaluation results and determines the deployment strategy and steps, including IT processes, deployment mode, and location.","Option B (Monitoring and maintenance plan) is tempting because it sounds operational, but it is about ongoing monitoring after deployment, not deployment strategy and IT process planning.", - 
CPMAI-P6-20260226-5931,Phase VI - Model Operationalization,1,Model operationalization plan,2,"Which Phase VI task produces an artifact that summarizes the operationalization strategy (for example batch mode, microservice, real-time streaming, on premise, or cloud) and the steps to perform it?",Model Governance Framework,Produce final report,Operationalization plan,Model Performance results,C,"Operationalization plan is the task that creates the Operationalization plan artifact summarizing deployment mode, location, and steps.","Option D is tempting because it sounds like a prerequisite, but Model Performance results is a Phase V evaluation task, not a Phase VI operationalization task.", - 
CPMAI-P6-20260226-8762,Phase VI - Model Operationalization,2,Model monitoring and maintenance,3,"A city transit agency deploys a route optimization model and worries it may degrade quietly in day-to-day use. They want a detailed monitoring process plan that matches the deployment type, includes contingency procedures for model failure, and prevents long periods of incorrect model usage. In Phase VI, which task should the team perform?",Model Governance Framework,Monitoring and maintenance plan,Operationalization plan,Review project,B,"Monitoring and maintenance plan is the Phase VI task that creates a detailed monitoring process plan tailored to the deployment, including contingency planning for model failures.","Option C (Operationalization plan) is tempting because deployment mode is involved, but Operationalization plan is about putting the model into operation, not defining the ongoing monitoring strategy.", - 
CPMAI-P6-20260226-3095,Phase VI - Model Operationalization,1,Project Report,4,What is the primary purpose of the Project Report task group in Phase VI?,Model monitoring and maintenance,Determine Requirements for the Next Iteration,Model operationalization plan,To summarize the team's activities and produce documentation so learnings can be shared with future teams,D,Project Report exists to capture and communicate the project's work — through a final report and a retrospective review — so that organizational learning is preserved and future teams benefit from the experience.,"Option B (Determine Requirements for the Next Iteration) is tempting because it also includes documentation, but that task group focuses on deciding future iteration scope, not summarizing past project activities for organizational learning.", - 
CPMAI-P6-20260226-6614,Phase VI - Model Operationalization,2,Determine Requirements for the Next Iteration,5,"A manufacturer operationalizes a defect-detection model but users request improvements for a new product line. The team must list potential further iterations, weigh reasons for and against each, and decide what to do next based on remaining budget and resources. In Phase VI, which task should the team perform?",Model Iteration Approach,Determine next steps,Operationalization plan,Review project,B,Determine next steps is the Phase VI task that lists potential future iterations and makes a proceed decision influenced by remaining resources and budget.,"Option A is tempting because iteration is mentioned, but Model Iteration Approach is a Phase V concept focused on model iteration planning during evaluation, not Phase VI future-iteration scoping based on budget and resources.", - 
CPMAI-P6-20260226-4278,Phase VI - Model Operationalization,3,Model monitoring and maintenance,6,"After launch, a credit-risk model is used by multiple business teams and disputes arise about who can approve model changes. The organization needs named owners, a plan for how the governance team will solicit and evaluate feedback, and steps to address model modification requests. In Phase VI, which task should the team perform?",Model Governance Framework,Monitoring and maintenance plan,Produce final report,Determine next steps,A,Model Governance Framework defines ownership and the structure for how model modification and ongoing usage decisions will be made based on feedback.,"Option B (Monitoring and maintenance plan) is the trap because it sounds like ongoing oversight, but Monitoring and maintenance plan is about monitoring strategy and tools, not governance ownership and decision-making authority for model changes.","Governance defines decision ownership and feedback handling, while monitoring defines how performance is observed and maintained."
CPMAI-P6-20260226-9183,Phase VI - Model Operationalization,1,Model monitoring and maintenance,7,"Beyond tracking performance metrics, what additional responsibility does the Model monitoring and maintenance task group cover in Phase VI?",Defining the deployment mode and IT processes,Producing the final project report,Contingency planning for models that fail in operation,Determining whether to pursue another iteration,C,Model monitoring and maintenance is the task group focused on ensuring the model continues to provide expected results and includes contingency planning for models that fail in operation — not just metric tracking.,"Option A (defining deployment mode) is tempting because it sounds operational, but deployment mode and IT processes belong to the Model operationalization plan task group.", - 
CPMAI-P6-20260226-2750,Phase VI - Model Operationalization,2,Project Report,8,"An edtech company finishes an AI-powered tutoring recommendation project. The sponsor requests a comprehensive deliverable that compiles all prior phase artifacts, summarizes results, and includes a presentation for the executive board. A separate team member suggests simply documenting what went wrong. In Phase VI, which task produces the comprehensive deliverable the sponsor requested?",Review project,Produce final report,Operationalization plan,Model Governance Framework,B,Produce final report is the Phase VI task that creates the final report and presentation summarizing and organizing previous deliverables and project results.,"Option A (Review project) is tempting because it is also written documentation, but Review project is a post-mortem on what went well or poorly, not the full compilation of deliverables and results the sponsor requested.", - 
CPMAI-P6-20260226-8041,Phase VI - Model Operationalization,1,Determine Requirements for the Next Iteration,9,"According to CPMAI, what factor must Determine next steps explicitly analyze when deciding whether to pursue another project iteration?",Remaining resources and budget,The model's training data volume,The number of Phase V evaluation metrics that passed,The deployment mode selected in the Operationalization plan,A,Determine next steps includes analysis of remaining resources and budget to influence the decision on how to proceed after assessment and process review.,"Option C is tempting because Phase V results inform the decision, but the specific factor Determine next steps explicitly analyzes is remaining resources and budget, not a count of passed metrics.", - 
CPMAI-P6-20260226-5169,Phase VI - Model Operationalization,2,Model operationalization plan,10,"A telecom company's Phase V evaluation approves a customer churn model for production. The infrastructure team asks whether to deploy the model as a nightly batch job or integrate it into the real-time CRM pipeline. The data science team needs to document the IT change management procedures and confirm the deployment location. In Phase VI, which task is responsible for answering these questions in its artifact?",Model Governance Framework,Monitoring and maintenance plan,Review process,Operationalization plan,D,Operationalization plan requires documenting the operationalization mode and location and the IT processes needed to put the model into operation.,"Option C is tempting because it mentions approvals, but Review process is a Phase V task, not the Phase VI deployment strategy task.", - 
CPMAI-P6-20260226-7392,Phase VI - Model Operationalization,1,Project Report,11,"In Phase VI, what type of documentation does Review project produce?",Determine next steps,"Experience documentation capturing what went right, what went wrong, pitfalls, misleading approaches, and improvement ideas",Monitoring and maintenance plan,Operationalization plan,B,"Review project is the Phase VI task that performs the iteration post-mortem and captures experience documentation including pitfalls, misleading approaches, and improvement ideas.","Option A (Determine next steps) is tempting because it also generates decisions, but Determine next steps focuses on selecting future iterations based on resources, not capturing retrospective experience documentation.", - 
CPMAI-P6-20260226-1846,Phase VI - Model Operationalization,3,Determine Requirements for the Next Iteration,12,"A sponsor says, ""We already wrote a post-mortem, so we are done deciding what to do next."" The team still needs to choose between multiple possible follow-on iterations and justify the decision based on remaining resources and budget. In Phase VI, which task addresses the team's remaining need?",Review project,Produce final report,Determine next steps,Monitoring and maintenance plan,C,"Determine next steps is where the team lists potential further iterations, weighs reasons, and makes a proceed decision influenced by resources and budget.","Option A (Review project) is the trap because it captures learnings, but Review project documents experience and improvements rather than selecting and justifying future iterations based on resources.",Decision-and-rationale for future iterations is distinct from a post-mortem experience review.
CPMAI-P6-20260226-6507,Phase VI - Model Operationalization,1,Model monitoring and maintenance,13,"Which Phase VI task produces a Monitoring and maintenance plan artifact that summarizes the monitoring and maintenance strategy, including steps and how to perform them?",Model Governance Framework,Operationalization plan,Determine next steps,Monitoring and maintenance plan,D,Monitoring and maintenance plan is the task that creates the Monitoring and maintenance plan artifact describing the monitoring approach and maintenance strategy.,"Option A (Model Governance Framework) is tempting because governance is also ongoing, but it defines ownership and decision authority, not the monitoring strategy and maintenance steps.", - 
CPMAI-P6-20260226-2901,Phase VI - Model Operationalization,2,Model monitoring and maintenance,14,"A healthcare provider deploys a triage-support model and wants a formal structure defining who owns the model, how user feedback will be collected, how concerns will be evaluated, and what steps will be taken when modification requests arise. In Phase VI, which task should the team perform?",Model Governance Framework,Monitoring and maintenance plan,Operationalization plan,Produce final report,A,"Model Governance Framework defines the governance team operation, feedback handling, and ownership for ongoing model usage and modification decisions.","Option B (Monitoring and maintenance plan) is tempting because it is also ongoing, but Monitoring and maintenance plan is about monitoring strategy and tools, not governance decision authority and feedback handling.", - 
CPMAI-P6-20260226-9654,Phase VI - Model Operationalization,1,Model operationalization plan,15,"Within Phase VI, what does ""model scaffolding"" in the operationalization work describe?","Additional application development, coding, or other non-AI work needed to put the model into a place where it can be used",The statistical method for detecting model drift in production,The process of selecting the modeling technique and initial hyperparameters,The final written report that includes all previous deliverables,A,"Model scaffolding describes the extra non-AI work required to operationalize the model, which may require additional user stories or sprint cycles in an Agile approach.","Option D is tempting because it is also Phase VI documentation, but the final report belongs to Produce final report, not model scaffolding within operationalization.", - 
CPMAI-P6-20260226-4320,Phase VI - Model Operationalization,3,Project Report,16,"A stakeholder asks for a deliverable that compiles and organizes all prior project deliverables and summarizes the project results for presentation. Another stakeholder asks for a separate document focused on what went wrong and what to improve next time. In Phase VI, which task addresses the first stakeholder's request?",Review project,Determine next steps,Produce final report,Operationalization plan,C,Produce final report creates the final report and presentation that includes and organizes previous deliverables and summarizes results.,"Option A (Review project) is the trap because it is also a written deliverable, but Review project is a post-mortem experience document rather than a comprehensive compilation and presentation of all project deliverables and results.",Comprehensive report of deliverables and results differs from post-mortem experience documentation.
CPMAI-P6-20260226-7108,Phase VI - Model Operationalization,1,Project Report,17,"In Phase VI, what determines whether Produce final report creates a brief summary or a comprehensive presentation of project results?",The number of phases the project completed,The regulatory requirements of the industry,The model's final accuracy score,The operationalization plan — the report scope depends on the deployment context,D,"Produce final report writes a final report that may be a summary or a comprehensive presentation of the project results, depending on the operationalization plan and deployment context.","Option A is tempting because longer projects might seem to warrant longer reports, but CPMAI ties the report scope to the operationalization plan, not to the number of phases completed.", - 
CPMAI-P6-20260226-5586,Phase VI - Model Operationalization,2,Determine Requirements for the Next Iteration,18,"A retail team completed operationalization and monitoring setup, but early performance feedback suggests a new iteration may be needed. The team must list possible further iterations, document reasons for and against each option, and make a decision with rationale considering budget constraints. In Phase VI, which task should the team perform?",Model Governance Framework,Determine next steps,Monitoring and maintenance plan,Operationalization plan,B,"Determine next steps requires listing potential further iterations and making a decision on how to proceed with rationale, influenced by remaining resources and budget.","Option A (Model Governance Framework) is tempting because it involves decisions, but Model Governance Framework is about ongoing governance structure and ownership, not selecting future iteration scope based on budget and resources.", - 
CPMAI-P6-20260226-1279,Phase VI - Model Operationalization,2,Model monitoring and maintenance,19,"A cybersecurity team has documented its monitoring approach in a detailed plan. The plan specifies alert thresholds, escalation procedures, and contingency responses for model failure. The team now needs to put that monitoring and management into operation exactly as planned. In Phase VI, which task includes implementing the monitoring and management approach?",Operationalization plan,Produce final report,Monitoring and maintenance plan,Review project,C,Monitoring and maintenance plan includes implementing the monitoring and management as detailed in the plan — both creating the plan and executing it.,"Option A (Operationalization plan) is tempting because it is about operating the model, but Operationalization plan focuses on deploying the model and related IT processes, not implementing the ongoing monitoring approach.", - 
CPMAI-P6-20260226-8435,Phase VI - Model Operationalization,3,Model operationalization plan,20,"A project manager drafts the Phase VI Operationalization plan and includes two items: (1) the IT processes to follow and the deployment mode and location, and (2) a list of required approvals and reviews that must occur before production. Which item correctly belongs in the Operationalization plan?",Detail required approvals and reviews that must occur before the model can be operationalized,Select the model iteration approach based on evaluation findings,Define the business KPI thresholds that must be met to justify rollout,Document the IT processes and the operationalization mode and location for deployment,D,Operationalization plan is responsible for documenting deployment mode and location and the IT processes needed to operationalize the model.,"Option A is the trap because it sounds like readiness work, but detailing required approvals and reviews is part of the Phase V Review process, not the Phase VI Operationalization plan.","Operationalization plan documents how the model will be deployed and the IT steps, while Phase V Review process handles required approvals and pre-deployment reviews."